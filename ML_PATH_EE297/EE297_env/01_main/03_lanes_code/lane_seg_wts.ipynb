{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7450c338-5eaa-4ad8-99e8-3b642e487765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e119a4b-ec76-4a35-b92b-4b150c9a3360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a561caa1-c124-4143-82cc-e6078f52cb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205fbbb-9b09-4cbc-acf7-46920552871b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad177fbc-b76b-4732-815e-81729dd76644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Necessary Liobraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "from tqdm.auto import tqdm as notebook_tqdm\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, AveragePooling2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#import torchvision.transforms as transforms\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fa35d-ef94-44b1-8bab-e4bb0ffd7d99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RESIZING IMAGE (SW + HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc51230-f090-4c18-b5d2-a56b0210457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Input and output paths\n",
    "input_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og.jpg\"\n",
    "output_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(input_path)\n",
    "\n",
    "# Resize to 224x224\n",
    "resized_image = image.resize((224, 224))\n",
    "\n",
    "# Save the resized image\n",
    "resized_image.save(output_path)\n",
    "\n",
    "print(f\"Image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2efae-d8a8-4f97-8778-76fea27c721a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CONVERTS IMAGE TO .TXT FILE (HW) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "55cefa19-5039-4f71-b19b-10e87b652a76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3def049c-fca4-4dd4-a235-39864a60dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Normalized image written to .txt for hardware input:\n",
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_rgb_224.txt\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== USER INPUT ====\n",
    "input_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\"\n",
    "output_basename = \"img_rgb_224.txt\"  # Output .txt filename\n",
    "target_size = (224, 224)             # Resize target\n",
    "\n",
    "# ==== LOAD IMAGE ====\n",
    "img = cv2.imread(input_path)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Could not load image at: {input_path}\")\n",
    "\n",
    "img = cv2.resize(img, target_size)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR → RGB\n",
    "img = img.astype(np.float32) / 255.0        # Normalize to [0,1]\n",
    "\n",
    "# ==== NORMALIZE WITH MEAN & STD (ImageNet-style) ====\n",
    "mean = np.array([0.485, 0.456, 0.406])  # R, G, B\n",
    "std  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "img = (img - mean[None, None, :]) / std[None, None, :]  # broadcast across H, W\n",
    "\n",
    "# ==== WRITE TO TXT ====\n",
    "output_dir = os.path.dirname(input_path)\n",
    "output_path = os.path.join(output_dir, output_basename)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    for c in range(3):  # R, G, B\n",
    "        for i in range(target_size[1]):\n",
    "            for j in range(target_size[0]):\n",
    "                f.write(f\"{img[i, j, c]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "print(f\"[✓] Normalized image written to .txt for hardware input:\\n{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09eebd-7a8b-4f26-9107-5328fd256dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MODEL CREATION (SW)\n",
    "    1. LOAD IN TRAINED WEIGHTS FILE (SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b318cd-955a-4e69-9431-4c4d5b4c1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class LaneSegOnlyMobileNetV2(nn.Module):\n",
    "    def __init__(self, pretrained=True, freeze_stem=False):\n",
    "        super().__init__()\n",
    "        # ---- Encoder: MobileNetV2 ----\n",
    "        weights = models.MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        m = models.mobilenet_v2(weights=weights)\n",
    "\n",
    "        # Feature extractor (ends at last conv-bn-relu block)\n",
    "        self.encoder = m.features              # output: (B, 1280, 7, 7) for 224x224\n",
    "        enc_out_ch = 1280\n",
    "\n",
    "        if freeze_stem:\n",
    "            # freeze early, cheap speed/regularization trick\n",
    "            for p in list(self.encoder.parameters())[:]:\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # ---- Lightweight decoder (no skip connections) ----\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(enc_out_ch, 256, kernel_size=2, stride=2),  # 7 -> 14\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),         # 14 -> 28\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),          # 28 -> 56\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),           # 56 -> 112\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 1, kernel_size=1),  # logits (B,1,112,112)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = x.shape[-2:]\n",
    "        feats = self.encoder(x)                 # (B,1280,7,7) at 224x224\n",
    "        seg   = self.seg_head(feats)           # (B,1,112,112)\n",
    "        seg   = F.interpolate(seg, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        return seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e33fb-c65f-45c6-8f03-1028412109ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45bf193c-cd33-4cf9-b954-5aa2985c594b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_27156\\2694822331.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LaneSegOnlyMobileNetV2(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (seg_head): Sequential(\n",
       "    (0): ConvTranspose2d(1280, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LaneSegOnlyMobileNetV2()\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # set to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e58870c-37f4-48d9-9d03-af2bfbe36c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.0.weight torch.Size([32, 3, 3, 3])\n",
      "encoder.0.1.weight torch.Size([32])\n",
      "encoder.0.1.bias torch.Size([32])\n",
      "encoder.1.conv.0.0.weight torch.Size([32, 1, 3, 3])\n",
      "encoder.1.conv.0.1.weight torch.Size([32])\n",
      "encoder.1.conv.0.1.bias torch.Size([32])\n",
      "encoder.1.conv.1.weight torch.Size([16, 32, 1, 1])\n",
      "encoder.1.conv.2.weight torch.Size([16])\n",
      "encoder.1.conv.2.bias torch.Size([16])\n",
      "encoder.2.conv.0.0.weight torch.Size([96, 16, 1, 1])\n",
      "encoder.2.conv.0.1.weight torch.Size([96])\n",
      "encoder.2.conv.0.1.bias torch.Size([96])\n",
      "encoder.2.conv.1.0.weight torch.Size([96, 1, 3, 3])\n",
      "encoder.2.conv.1.1.weight torch.Size([96])\n",
      "encoder.2.conv.1.1.bias torch.Size([96])\n",
      "encoder.2.conv.2.weight torch.Size([24, 96, 1, 1])\n",
      "encoder.2.conv.3.weight torch.Size([24])\n",
      "encoder.2.conv.3.bias torch.Size([24])\n",
      "encoder.3.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "encoder.3.conv.0.1.weight torch.Size([144])\n",
      "encoder.3.conv.0.1.bias torch.Size([144])\n",
      "encoder.3.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "encoder.3.conv.1.1.weight torch.Size([144])\n",
      "encoder.3.conv.1.1.bias torch.Size([144])\n",
      "encoder.3.conv.2.weight torch.Size([24, 144, 1, 1])\n",
      "encoder.3.conv.3.weight torch.Size([24])\n",
      "encoder.3.conv.3.bias torch.Size([24])\n",
      "encoder.4.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "encoder.4.conv.0.1.weight torch.Size([144])\n",
      "encoder.4.conv.0.1.bias torch.Size([144])\n",
      "encoder.4.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "encoder.4.conv.1.1.weight torch.Size([144])\n",
      "encoder.4.conv.1.1.bias torch.Size([144])\n",
      "encoder.4.conv.2.weight torch.Size([32, 144, 1, 1])\n",
      "encoder.4.conv.3.weight torch.Size([32])\n",
      "encoder.4.conv.3.bias torch.Size([32])\n",
      "encoder.5.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.5.conv.0.1.weight torch.Size([192])\n",
      "encoder.5.conv.0.1.bias torch.Size([192])\n",
      "encoder.5.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.5.conv.1.1.weight torch.Size([192])\n",
      "encoder.5.conv.1.1.bias torch.Size([192])\n",
      "encoder.5.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "encoder.5.conv.3.weight torch.Size([32])\n",
      "encoder.5.conv.3.bias torch.Size([32])\n",
      "encoder.6.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.6.conv.0.1.weight torch.Size([192])\n",
      "encoder.6.conv.0.1.bias torch.Size([192])\n",
      "encoder.6.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.6.conv.1.1.weight torch.Size([192])\n",
      "encoder.6.conv.1.1.bias torch.Size([192])\n",
      "encoder.6.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "encoder.6.conv.3.weight torch.Size([32])\n",
      "encoder.6.conv.3.bias torch.Size([32])\n",
      "encoder.7.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.7.conv.0.1.weight torch.Size([192])\n",
      "encoder.7.conv.0.1.bias torch.Size([192])\n",
      "encoder.7.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.7.conv.1.1.weight torch.Size([192])\n",
      "encoder.7.conv.1.1.bias torch.Size([192])\n",
      "encoder.7.conv.2.weight torch.Size([64, 192, 1, 1])\n",
      "encoder.7.conv.3.weight torch.Size([64])\n",
      "encoder.7.conv.3.bias torch.Size([64])\n",
      "encoder.8.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.8.conv.0.1.weight torch.Size([384])\n",
      "encoder.8.conv.0.1.bias torch.Size([384])\n",
      "encoder.8.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.8.conv.1.1.weight torch.Size([384])\n",
      "encoder.8.conv.1.1.bias torch.Size([384])\n",
      "encoder.8.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.8.conv.3.weight torch.Size([64])\n",
      "encoder.8.conv.3.bias torch.Size([64])\n",
      "encoder.9.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.9.conv.0.1.weight torch.Size([384])\n",
      "encoder.9.conv.0.1.bias torch.Size([384])\n",
      "encoder.9.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.9.conv.1.1.weight torch.Size([384])\n",
      "encoder.9.conv.1.1.bias torch.Size([384])\n",
      "encoder.9.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.9.conv.3.weight torch.Size([64])\n",
      "encoder.9.conv.3.bias torch.Size([64])\n",
      "encoder.10.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.10.conv.0.1.weight torch.Size([384])\n",
      "encoder.10.conv.0.1.bias torch.Size([384])\n",
      "encoder.10.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.10.conv.1.1.weight torch.Size([384])\n",
      "encoder.10.conv.1.1.bias torch.Size([384])\n",
      "encoder.10.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.10.conv.3.weight torch.Size([64])\n",
      "encoder.10.conv.3.bias torch.Size([64])\n",
      "encoder.11.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.11.conv.0.1.weight torch.Size([384])\n",
      "encoder.11.conv.0.1.bias torch.Size([384])\n",
      "encoder.11.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.11.conv.1.1.weight torch.Size([384])\n",
      "encoder.11.conv.1.1.bias torch.Size([384])\n",
      "encoder.11.conv.2.weight torch.Size([96, 384, 1, 1])\n",
      "encoder.11.conv.3.weight torch.Size([96])\n",
      "encoder.11.conv.3.bias torch.Size([96])\n",
      "encoder.12.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.12.conv.0.1.weight torch.Size([576])\n",
      "encoder.12.conv.0.1.bias torch.Size([576])\n",
      "encoder.12.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.12.conv.1.1.weight torch.Size([576])\n",
      "encoder.12.conv.1.1.bias torch.Size([576])\n",
      "encoder.12.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "encoder.12.conv.3.weight torch.Size([96])\n",
      "encoder.12.conv.3.bias torch.Size([96])\n",
      "encoder.13.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.13.conv.0.1.weight torch.Size([576])\n",
      "encoder.13.conv.0.1.bias torch.Size([576])\n",
      "encoder.13.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.13.conv.1.1.weight torch.Size([576])\n",
      "encoder.13.conv.1.1.bias torch.Size([576])\n",
      "encoder.13.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "encoder.13.conv.3.weight torch.Size([96])\n",
      "encoder.13.conv.3.bias torch.Size([96])\n",
      "encoder.14.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.14.conv.0.1.weight torch.Size([576])\n",
      "encoder.14.conv.0.1.bias torch.Size([576])\n",
      "encoder.14.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.14.conv.1.1.weight torch.Size([576])\n",
      "encoder.14.conv.1.1.bias torch.Size([576])\n",
      "encoder.14.conv.2.weight torch.Size([160, 576, 1, 1])\n",
      "encoder.14.conv.3.weight torch.Size([160])\n",
      "encoder.14.conv.3.bias torch.Size([160])\n",
      "encoder.15.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.15.conv.0.1.weight torch.Size([960])\n",
      "encoder.15.conv.0.1.bias torch.Size([960])\n",
      "encoder.15.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.15.conv.1.1.weight torch.Size([960])\n",
      "encoder.15.conv.1.1.bias torch.Size([960])\n",
      "encoder.15.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "encoder.15.conv.3.weight torch.Size([160])\n",
      "encoder.15.conv.3.bias torch.Size([160])\n",
      "encoder.16.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.16.conv.0.1.weight torch.Size([960])\n",
      "encoder.16.conv.0.1.bias torch.Size([960])\n",
      "encoder.16.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.16.conv.1.1.weight torch.Size([960])\n",
      "encoder.16.conv.1.1.bias torch.Size([960])\n",
      "encoder.16.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "encoder.16.conv.3.weight torch.Size([160])\n",
      "encoder.16.conv.3.bias torch.Size([160])\n",
      "encoder.17.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.17.conv.0.1.weight torch.Size([960])\n",
      "encoder.17.conv.0.1.bias torch.Size([960])\n",
      "encoder.17.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.17.conv.1.1.weight torch.Size([960])\n",
      "encoder.17.conv.1.1.bias torch.Size([960])\n",
      "encoder.17.conv.2.weight torch.Size([320, 960, 1, 1])\n",
      "encoder.17.conv.3.weight torch.Size([320])\n",
      "encoder.17.conv.3.bias torch.Size([320])\n",
      "encoder.18.0.weight torch.Size([1280, 320, 1, 1])\n",
      "encoder.18.1.weight torch.Size([1280])\n",
      "encoder.18.1.bias torch.Size([1280])\n",
      "seg_head.0.weight torch.Size([1280, 256, 2, 2])\n",
      "seg_head.0.bias torch.Size([256])\n",
      "seg_head.1.weight torch.Size([256])\n",
      "seg_head.1.bias torch.Size([256])\n",
      "seg_head.3.weight torch.Size([256, 256, 3, 3])\n",
      "seg_head.3.bias torch.Size([256])\n",
      "seg_head.4.weight torch.Size([256])\n",
      "seg_head.4.bias torch.Size([256])\n",
      "seg_head.6.weight torch.Size([256, 128, 2, 2])\n",
      "seg_head.6.bias torch.Size([128])\n",
      "seg_head.7.weight torch.Size([128])\n",
      "seg_head.7.bias torch.Size([128])\n",
      "seg_head.9.weight torch.Size([128, 128, 3, 3])\n",
      "seg_head.9.bias torch.Size([128])\n",
      "seg_head.10.weight torch.Size([128])\n",
      "seg_head.10.bias torch.Size([128])\n",
      "seg_head.12.weight torch.Size([128, 64, 2, 2])\n",
      "seg_head.12.bias torch.Size([64])\n",
      "seg_head.13.weight torch.Size([64])\n",
      "seg_head.13.bias torch.Size([64])\n",
      "seg_head.15.weight torch.Size([64, 64, 3, 3])\n",
      "seg_head.15.bias torch.Size([64])\n",
      "seg_head.16.weight torch.Size([64])\n",
      "seg_head.16.bias torch.Size([64])\n",
      "seg_head.18.weight torch.Size([64, 32, 2, 2])\n",
      "seg_head.18.bias torch.Size([32])\n",
      "seg_head.19.weight torch.Size([32])\n",
      "seg_head.19.bias torch.Size([32])\n",
      "seg_head.21.weight torch.Size([32, 32, 3, 3])\n",
      "seg_head.21.bias torch.Size([32])\n",
      "seg_head.22.weight torch.Size([32])\n",
      "seg_head.22.bias torch.Size([32])\n",
      "seg_head.24.weight torch.Size([1, 32, 1, 1])\n",
      "seg_head.24.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a22b9-babe-40f0-9cd2-5dd5d464c93f",
   "metadata": {},
   "source": [
    "#### =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b21c8f-5207-43a6-9139-040d6ae84f44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (encoder): Sequential( HW Conv -> FOLD (BatchNorm) -> Relu)\n",
    "1. Save Weights File (HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66cf1fbd-dea3-46c7-9f07-da325ec4d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv shape (O,I,K,K): (32, 3, 3, 3)\n",
      "BN shapes: (32,) (32,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#    It's a Conv2dNormActivation with (0)=Conv2d, (1)=BatchNorm2d, (2)=ReLU6\n",
    "# encoder[0] = Conv2dNormActivation: [0]=Conv2d(bias=False), [1]=BatchNorm2d, [2]=ReLU6\n",
    "enc0 = model.encoder[0]\n",
    "conv: nn.Conv2d = enc0[0]\n",
    "bn:   nn.BatchNorm2d = enc0[1]\n",
    "\n",
    "assert isinstance(conv, nn.Conv2d)\n",
    "assert isinstance(bn,   nn.BatchNorm2d)\n",
    "\n",
    "print(\"Conv shape (O,I,K,K):\", tuple(conv.weight.shape))\n",
    "print(\"BN shapes:\", tuple(bn.weight.shape), tuple(bn.bias.shape))\n",
    "\n",
    "# Setting up BatchNorm Math Fold:\n",
    "# Get tensors (PyTorch conv layout: [O,I,K,K])\n",
    "W = conv.weight.detach().cpu().numpy().astype(\"float32\")          # (O,I,K,K)\n",
    "gamma = bn.weight.detach().cpu().numpy().astype(\"float32\")        # (O,)\n",
    "beta  = bn.bias.detach().cpu().numpy().astype(\"float32\")          # (O,)\n",
    "mean  = bn.running_mean.detach().cpu().numpy().astype(\"float32\")  # (O,)\n",
    "var   = bn.running_var.detach().cpu().numpy().astype(\"float32\")   # (O,)\n",
    "eps   = float(bn.eps)\n",
    "\n",
    "# Per-output scale\n",
    "s = (gamma / np.sqrt(var + eps)).astype(\"float32\")  # (O,)\n",
    "\n",
    "# Fold into conv (bias=False case)\n",
    "W_fold = (W * s[:, None, None, None]).astype(\"float32\")  # broadcast scale over I,K,K\n",
    "b_fold = (beta - mean * s).astype(\"float32\")             # (O,)\n",
    "\n",
    "# If your HLS expects (K,K,IN_C,OUT_C) layout:\n",
    "W_hls = np.transpose(W_fold, (2, 3, 1, 0))  # (K,K,I,O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121a2fbb-db90-48d8-a435-29d183295eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\encoder_conv0_w.h\n",
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\encoder_conv0_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"      # relative path in your project\n",
    "CTYPE   = \"data_t\"       # ap_fixed typedef in your HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Writes a header with:\n",
    "      - #pragma once\n",
    "      - your include path\n",
    "      - declaration: data_t var[... ] = { ... };\n",
    "      - numbers printed as plain literals (no 'f', no '(data_t)' casts)\n",
    "    \"\"\"\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            # no 'f' and no casts; compact scientific where needed\n",
    "            return \"{\" + \",\".join(f\"{x:.8f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f'{HEADER_INCLUDE}\\n\\n')\n",
    "        f.write(\"// Auto-generated from encoder[0] with BN folded\\n\\n\")\n",
    "        # no 'const' here\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    # print full path (one-liner)\n",
    "    print(os.path.abspath(out_path))\n",
    "\n",
    "# ---- call these with your already-computed arrays ----\n",
    "# W_hls: (K,K,IN_C,OUT_C), b_fold: (OUT_C,)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "write_header_plain_numbers(W_hls, \"conv0_w\", CTYPE, os.path.join(OUT_DIR, \"encoder_conv0_w.h\"))\n",
    "write_header_plain_numbers(b_fold, \"conv0_b\", CTYPE, os.path.join(OUT_DIR, \"encoder_conv0_b.h\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c4cd2-9fea-43c0-a8c5-4675c0ae4b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee8292cf-76d1-4776-92e2-a9e3b13b949f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (encoder): SW output of Conv Stage 0 Comparison\n",
    "     1. Actual Model Software Output\n",
    "     2. Custom Stage0 software output mimic data_t\n",
    "     3. Print Debug Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248ee22-2ad4-4a8b-8e2b-4b0ea9f974a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "INT_BITS = 4\n",
    "FRAC_BITS = 12\n",
    "IMG_PATH = \"C:/Users/Baron/Desktop/EE_297_Repo/EE_297/hardware_imp/vitis_hls/lane_seg_hls/test_imgs/img_rgb_224.txt\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# FIXED-POINT QUANTIZATION\n",
    "# -------------------------------\n",
    "def quantize_fixed(x, int_bits=INT_BITS, frac_bits=FRAC_BITS):\n",
    "    scale = 2 ** frac_bits\n",
    "    max_val = (2 ** (int_bits - 1)) - 1 / scale\n",
    "    min_val = -(2 ** (int_bits - 1))\n",
    "    return np.clip(np.round(x * scale) / scale, min_val, max_val)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD IMAGE (.txt format, channel-major: R→G→B blocks)\n",
    "# -------------------------------\n",
    "def load_txt_tensor(filepath, H=224, W=224, C=3):\n",
    "    arr = np.loadtxt(filepath, dtype=np.float32)\n",
    "    if arr.size != C * H * W:\n",
    "        raise ValueError(f\"Unexpected image size: got {arr.size} elements\")\n",
    "\n",
    "    arr = arr.reshape(C, H, W)       # (C,H,W), channel-major\n",
    "    arr = np.expand_dims(arr, axis=0)  # (1,C,H,W) for torch\n",
    "    return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN PIPELINE\n",
    "# -------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[0] weights and BN params\n",
    "enc0 = model.encoder[0]\n",
    "conv: torch.nn.Conv2d = enc0[0]\n",
    "bn:   torch.nn.BatchNorm2d = enc0[1]\n",
    "\n",
    "# Fold BN into conv weights\n",
    "W = conv.weight.detach().cpu().numpy().astype(np.float32)         # (O,I,K,K)\n",
    "gamma = bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta  = bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean  = bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var   = bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps   = float(bn.eps)\n",
    "\n",
    "s = gamma / np.sqrt(var + eps)\n",
    "W_fold = W * s[:, None, None, None]\n",
    "b_fold = beta - mean * s\n",
    "\n",
    "# Quantize weights and biases\n",
    "W_fold_q = quantize_fixed(W_fold)\n",
    "b_fold_q = quantize_fixed(b_fold)\n",
    "W_torch = torch.tensor(W_fold_q).to(device)\n",
    "b_torch = torch.tensor(b_fold_q).to(device)\n",
    "\n",
    "# Load image directly from .txt\n",
    "x = load_txt_tensor(IMG_PATH).to(device)\n",
    "\n",
    "# Quantize input\n",
    "x_q = quantize_fixed(x.cpu().numpy())\n",
    "x_q = torch.tensor(x_q).to(device)\n",
    "\n",
    "# Run convolution + ReLU6\n",
    "#with torch.no_grad():\n",
    "#    y = F.conv2d(x_q, W_torch, bias=b_torch, stride=2, padding=1)\n",
    "# y = quantize_fixed(y.cpu().numpy())\n",
    "#    y = torch.clamp(y, 0, 6)\n",
    "\n",
    "import math\n",
    "\n",
    "def add_mac_noise(y, kH, kW, in_ch, frac_bits=12):\n",
    "    \"\"\"\n",
    "    Approximate ap_fixed<16,4> quantization loss after MAC accumulation.\n",
    "    \n",
    "    Args:\n",
    "        y: torch.Tensor, (B, C_out, H_out, W_out) - float result of conv2d\n",
    "        kH, kW: kernel height and width\n",
    "        in_ch: number of input channels\n",
    "        frac_bits: number of fractional bits (default=12 for ap_fixed<16,4>)\n",
    "    \"\"\"\n",
    "    # Quantization step\n",
    "    q = 2.0 ** (-frac_bits)   # e.g., 0.000244140625\n",
    "    # Number of MACs per output element\n",
    "    N = kH * kW * in_ch\n",
    "    # Stddev of accumulated quantization error\n",
    "    sigma = math.sqrt(N) * q / math.sqrt(12.0)\n",
    "\n",
    "    # Add Gaussian noise to approximate rounding loss\n",
    "    noise = torch.randn_like(y) * sigma\n",
    "    return y + noise\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Modified forward block\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Normal float conv\n",
    "    y = F.conv2d(x_q, W_torch, bias=b_torch, stride=2, padding=1)\n",
    "    \n",
    "    # Inject approximate MAC quantization noise\n",
    "    kH, kW = 3, 3   # kernel size\n",
    "    in_ch = x_q.shape[1]\n",
    "    y = add_mac_noise(y, kH, kW, in_ch, frac_bits=FRAC_BITS)\n",
    "\n",
    "    # Apply ReLU6 and clamp\n",
    "    y = torch.clamp(y, 0, 6)\n",
    "\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder0_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder0_output.txt\"), \"w\") as f:\n",
    "    for c in range(32):\n",
    "        for i in range(112):\n",
    "            for j in range(112):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output with fixed-point weights, input, and arithmetic saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c506b-9461-49a3-9d5b-cfc6a0edee72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb7e25c-6b68-487d-b195-50f34e195a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: bias[0] = -0.022705\n",
      "DEBUG: weights[0][0][0][0] = 0.006592\n",
      "DEBUG: a=0.000000 * b=0.006592 => 0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=0.003662 => 0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=-0.016113 => -0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=-0.003418 => -0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=-0.004395 => -0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=-0.012939 => -0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=0.007568 => 0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=0.007812 => 0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=-0.006836 => -0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=0.018555 => 0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=0.010986 => 0.000000 | sum=-0.022705\n",
      "DEBUG: a=0.000000 * b=-0.007568 => -0.000000 | sum=-0.022705\n",
      "DEBUG: a=-1.501465 * b=-0.015869 => 0.023926 | sum=0.001221\n",
      "DEBUG: a=-1.265381 * b=-0.020264 => 0.025635 | sum=0.026855\n",
      "DEBUG: a=-1.002686 * b=-0.018311 => 0.018311 | sum=0.045166\n",
      "DEBUG: a=-1.518555 * b=0.003174 => -0.004883 | sum=0.040283\n",
      "DEBUG: a=-1.282959 * b=-0.002197 => 0.002930 | sum=0.043213\n",
      "DEBUG: a=-1.020020 * b=-0.010010 => 0.010254 | sum=0.053467\n",
      "DEBUG: a=0.000000 * b=0.005371 => 0.000000 | sum=0.053467\n",
      "DEBUG: a=0.000000 * b=0.005859 => 0.000000 | sum=0.053467\n",
      "DEBUG: a=0.000000 * b=-0.016602 => -0.000000 | sum=0.053467\n",
      "DEBUG: a=-1.689697 * b=-0.022949 => 0.038818 | sum=0.092285\n",
      "DEBUG: a=-1.458008 * b=-0.020508 => 0.029785 | sum=0.122070\n",
      "DEBUG: a=-1.194336 * b=-0.020752 => 0.024902 | sum=0.146973\n",
      "DEBUG: a=-1.672607 * b=-0.009521 => 0.015869 | sum=0.162842\n",
      "DEBUG: a=-1.440430 * b=-0.005371 => 0.007812 | sum=0.170654\n",
      "DEBUG: a=-1.177002 * b=-0.015137 => 0.017822 | sum=0.188477\n",
      "DEBUG: output[0][0][0] = 0.188477\n"
     ]
    }
   ],
   "source": [
    "#3. Print Debug Statements\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------------------\n",
    "# Step 1: Create HLS-style padded buffer\n",
    "# -----------------------------------------\n",
    "def pad_like_hls_layout(x, pad=1):\n",
    "    \"\"\"\n",
    "    Mimics HLS padded[y+PAD][x+PAD][c] = input[y][x][c]\n",
    "    Input:  x (1,C,H,W)\n",
    "    Output: (H+2*pad, W+2*pad, C) with channels innermost\n",
    "    \"\"\"\n",
    "    _, C, H, W = x.shape\n",
    "    out = np.zeros((H + 2*pad, W + 2*pad, C), dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        out[pad:pad+H, pad:pad+W, c] = x[0, c, :, :].cpu().numpy()\n",
    "    return out\n",
    "\n",
    "PAD = 1\n",
    "x_padded_hls = pad_like_hls_layout(x_q, pad=PAD)   # shape (226,226,3)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Step 2: Debug the very first conv output: [oh=0, ow=0, oc=0]\n",
    "# -----------------------------------------\n",
    "STRIDE = 2\n",
    "K = 3\n",
    "iy = 0 * STRIDE\n",
    "ix = 0 * STRIDE\n",
    "oc = 0\n",
    "\n",
    "\n",
    "bias_debug = quantize_fixed(b_torch[oc].item())\n",
    "print(f\"DEBUG: bias[0] = {bias_debug:.6f}\")\n",
    "\n",
    "w_debug = quantize_fixed(W_torch[oc, 0, 0, 0].item())\n",
    "print(f\"DEBUG: weights[0][0][0][0] = {w_debug:.6f}\")\n",
    "\n",
    "sum_debug = bias_debug\n",
    "\n",
    "for ky in range(K):\n",
    "    for kx in range(K):\n",
    "        for ic in range(3):  # IN_C = 3\n",
    "            # Quantize input + weight\n",
    "            a = quantize_fixed(x_padded_hls[iy + ky, ix + kx, ic])\n",
    "            b = quantize_fixed(W_torch[oc, ic, ky, kx].item())\n",
    "            # Multiply then quantize\n",
    "            p = quantize_fixed(a * b)\n",
    "            # Accumulate then quantize\n",
    "            sum_debug = quantize_fixed(sum_debug + p)\n",
    "            print(f\"DEBUG: a={a:.6f} * b={b:.6f} => {p:.6f} | sum={sum_debug:.6f}\")\n",
    "\n",
    "# Final ReLU6 (quantized)\n",
    "sum_clamped = max(0, min(6, sum_debug))\n",
    "print(f\"DEBUG: output[0][0][0] = {sum_clamped:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ed796-e546-4797-aaa7-9028434faf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f9059-ce3a-43dd-8ac6-f34b7df04fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51bdbd-3744-4abe-9572-ac5d82c24e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea94370-686c-47e1-86ee-e7cb978fcd8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual0): HW output of Conv Stage 1 \n",
    "         1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b36e63-b447-4395-a3a9-e2d5bb184322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depthwise conv shape (O,I,K,K): (32, 1, 3, 3)\n",
      "Depthwise BN shapes: (32,) (32,)\n",
      "Pointwise conv shape (O,I,K,K): (16, 32, 1, 1)\n",
      "Pointwise BN shapes: (16,) (16,)\n"
     ]
    }
   ],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 1)\n",
    "ir0 = model.encoder[1]        # InvertedResidual\n",
    "convseq = ir0.conv            # Sequential inside it\n",
    "\n",
    "# ---- Depthwise ConvNormActivation ----\n",
    "dw_conv: nn.Conv2d = convseq[0][0]   # Conv2d depthwise\n",
    "dw_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "assert isinstance(dw_conv, nn.Conv2d)\n",
    "assert isinstance(dw_bn,   nn.BatchNorm2d)\n",
    "\n",
    "print(\"Depthwise conv shape (O,I,K,K):\", tuple(dw_conv.weight.shape))\n",
    "print(\"Depthwise BN shapes:\", tuple(dw_bn.weight.shape), tuple(dw_bn.bias.shape))\n",
    "\n",
    "# Extract tensors\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")   # (O,I,K,K)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "# Fold BN into depthwise conv\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C)\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Pointwise Conv + BN ----\n",
    "pw_conv: nn.Conv2d = convseq[1]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[2]\n",
    "\n",
    "assert isinstance(pw_conv, nn.Conv2d)\n",
    "assert isinstance(pw_bn,   nn.BatchNorm2d)\n",
    "\n",
    "print(\"Pointwise conv shape (O,I,K,K):\", tuple(pw_conv.weight.shape))\n",
    "print(\"Pointwise BN shapes:\", tuple(pw_bn.weight.shape), tuple(pw_bn.bias.shape))\n",
    "\n",
    "# Extract tensors\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")   # (O,I,K,K)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "# Fold BN into pointwise conv\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — here K=1\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c9b1844-264a-4171-9ac2-c9084f26104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Depthwise\n",
    "write_header_plain_numbers(W_dw_hls, \"enc1_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc1_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc1_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc1_dw_b.h\"))\n",
    "\n",
    "# Pointwise\n",
    "write_header_plain_numbers(W_pw_hls, \"enc1_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc1_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc1_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc1_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220980e7-c5de-4765-ab26-7412358421d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual0): SW output of Conv Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82fb89ca-2a61-40e8-8636-eb6d539b6164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_20228\\941991276.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc0_ir0 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "INT_BITS = 4\n",
    "FRAC_BITS = 12\n",
    "STAGE0_OUT_PATH = \"software_output/sw_encoder0_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# FIXED-POINT QUANTIZATION\n",
    "# -------------------------------\n",
    "def quantize_fixed(x, int_bits=INT_BITS, frac_bits=FRAC_BITS):\n",
    "    scale = 2 ** frac_bits\n",
    "    max_val = (2 ** (int_bits - 1)) - 1 / scale\n",
    "    min_val = -(2 ** (int_bits - 1))\n",
    "    return np.clip(np.round(x * scale) / scale, min_val, max_val)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE0_OUT_PATH).astype(np.float32)   # shape (32,112,112)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,32,112,112)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[1] (InvertedResidual)\n",
    "ir0 = model.encoder[1]\n",
    "convseq = ir0.conv\n",
    "\n",
    "# ---- Depthwise Conv (with BN) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (32,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = W_dw * s_dw[:, None, None, None]  # broadcast\n",
    "b_dw_fold = beta_dw - mean_dw * s_dw\n",
    "\n",
    "# ---- Pointwise Conv (with BN) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[1]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[2]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (16,32,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = W_pw * s_pw[:, None, None, None]\n",
    "b_pw_fold = beta_pw - mean_pw * s_pw\n",
    "\n",
    "# ---- Quantize weights and biases ----\n",
    "W_dw_fold_q = quantize_fixed(W_dw_fold)\n",
    "b_dw_fold_q = quantize_fixed(b_dw_fold)\n",
    "W_pw_fold_q = quantize_fixed(W_pw_fold)\n",
    "b_pw_fold_q = quantize_fixed(b_pw_fold)\n",
    "\n",
    "W_dw_torch = torch.tensor(W_dw_fold_q).to(device)\n",
    "b_dw_torch = torch.tensor(b_dw_fold_q).to(device)\n",
    "W_pw_torch = torch.tensor(W_pw_fold_q).to(device)\n",
    "b_pw_torch = torch.tensor(b_pw_fold_q).to(device)\n",
    "\n",
    "# ---- Quantize input ----\n",
    "x_q = quantize_fixed(x)\n",
    "x_q = torch.tensor(x_q).to(device)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 1 (depthwise + pointwise)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    y = F.conv2d(x_q, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=32)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6 after depthwise\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (16,112,112)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder1_ir0_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder1_ir0_output.txt\"), \"w\") as f:\n",
    "    for c in range(16):\n",
    "        for i in range(112):\n",
    "            for j in range(112):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc0_ir0 saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee96dfb3-46c7-4057-88f4-9b4201e3b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG DW: bias[0] = -0.014160\n",
      "DW DEBUG: a=0.000000 * b=-0.068359 => -0.000000 | sum=-0.014160\n",
      "DW DEBUG: a=0.000000 * b=-0.081787 => -0.000000 | sum=-0.014160\n",
      "DW DEBUG: a=0.000000 * b=-0.067383 => -0.000000 | sum=-0.014160\n",
      "DW DEBUG: a=0.000000 * b=-0.137695 => -0.000000 | sum=-0.014160\n",
      "DW DEBUG: a=0.188232 * b=0.028564 => 0.005371 | sum=-0.008789\n",
      "DW DEBUG: a=0.153564 * b=0.760254 => 0.116699 | sum=0.107910\n",
      "DW DEBUG: a=0.000000 * b=-0.076904 => -0.000000 | sum=0.107910\n",
      "DW DEBUG: a=0.226562 * b=-0.062988 => -0.014160 | sum=0.093750\n",
      "DW DEBUG: a=0.179443 * b=0.057373 => 0.010254 | sum=0.104004\n",
      "DW DEBUG: output[0][0][0] after ReLU6 = 0.104004\n",
      "PW DEBUG: ic=0, a=0.104004, b=-0.010010, p=-0.000977, sum=-2.092773\n",
      "PW DEBUG: ic=1, a=0.000000, b=0.081299, p=0.000000, sum=-2.092773\n",
      "PW DEBUG: ic=2, a=0.000000, b=-0.066406, p=-0.000000, sum=-2.092773\n",
      "PW DEBUG: output[0][0][0] final = -2.092773\n"
     ]
    }
   ],
   "source": [
    "# Debug single output computation for enc0_ir0\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Step 1: Quantized padded input for depthwise conv\n",
    "def pad_like_hls_layout(x, pad=1):\n",
    "    _, C, H, W = x.shape\n",
    "    out = np.zeros((H + 2*pad, W + 2*pad, C), dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        out[pad:pad+H, pad:pad+W, c] = x[0, c, :, :].cpu().numpy()\n",
    "    return out\n",
    "\n",
    "PAD = 1\n",
    "x_padded_hls = pad_like_hls_layout(x_q, pad=PAD)  # (114,114,32)\n",
    "\n",
    "# Step 2: Debug the first pixel of depthwise\n",
    "STRIDE = 1\n",
    "K = 3\n",
    "iy, ix, oc = 0, 0, 0\n",
    "\n",
    "bias_debug = quantize_fixed(b_dw_torch[oc].item())\n",
    "print(f\"DEBUG DW: bias[0] = {bias_debug:.6f}\")\n",
    "\n",
    "sum_debug = bias_debug\n",
    "for ky in range(K):\n",
    "    for kx in range(K):\n",
    "        a = quantize_fixed(x_padded_hls[iy+ky, ix+kx, oc])\n",
    "        b = quantize_fixed(W_dw_torch[oc, 0, ky, kx].item())\n",
    "        p = quantize_fixed(a * b)\n",
    "        sum_debug = quantize_fixed(sum_debug + p)\n",
    "        print(f\"DW DEBUG: a={a:.6f} * b={b:.6f} => {p:.6f} | sum={sum_debug:.6f}\")\n",
    "\n",
    "sum_clamped = max(0, min(6, sum_debug))\n",
    "print(f\"DW DEBUG: output[0][0][{oc}] after ReLU6 = {sum_clamped:.6f}\")\n",
    "\n",
    "# Step 3: Debug pointwise conv for first output channel\n",
    "oc_pw = 0\n",
    "sum_pw = quantize_fixed(b_pw_torch[oc_pw].item())\n",
    "for ic in range(32):\n",
    "    a = quantize_fixed(sum_clamped if ic == 0 else 0)  # using only ic=0 for illustration\n",
    "    b = quantize_fixed(W_pw_torch[oc_pw, ic, 0, 0].item())\n",
    "    p = quantize_fixed(a * b)\n",
    "    sum_pw = quantize_fixed(sum_pw + p)\n",
    "    if ic < 3:\n",
    "        print(f\"PW DEBUG: ic={ic}, a={a:.6f}, b={b:.6f}, p={p:.6f}, sum={sum_pw:.6f}\")\n",
    "\n",
    "print(f\"PW DEBUG: output[0][0][{oc_pw}] final = {sum_pw:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdad7c-7591-4408-b913-7bb7567e20e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2fc02-a1a9-40cf-b27b-cbfa707f593a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6717de90-d53b-4235-9556-819338bdaff1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### UNUSED CODE"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36247900-de27-4139-97d0-002fe331a8b0",
   "metadata": {},
   "source": [
    "\n",
    "def load_txt_file(path):\n",
    "    \"\"\"\n",
    "    Load numbers from a text file.\n",
    "    Handles space, comma, or newline separated values.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    # Split by whitespace or commas\n",
    "    numbers = text.replace(\",\", \" \").split()\n",
    "    return np.array([float(x) for x in numbers], dtype=np.float64)\n",
    "\n",
    "def compare_outputs(hw_file, sw_file, dump_mismatches=False, tol=1e-5):\n",
    "    # Load both files\n",
    "    hw = load_txt_file(hw_file)\n",
    "    sw = load_txt_file(sw_file)\n",
    "\n",
    "    if hw.shape != sw.shape:\n",
    "        raise ValueError(f\"Shape mismatch: HW={hw.shape}, SW={sw.shape}\")\n",
    "\n",
    "    # Compute error metrics\n",
    "    abs_err = np.abs(hw - sw)\n",
    "    rel_err = abs_err / (np.abs(sw) + 1e-12)  # avoid div by zero\n",
    "\n",
    "    mse = np.mean(abs_err**2)\n",
    "    mae = np.mean(abs_err)\n",
    "    max_err = np.max(abs_err)\n",
    "\n",
    "    print(\"==== Quantization Error Report ====\")\n",
    "    print(f\"Number of elements: {hw.size}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.6e}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.6e}\")\n",
    "    print(f\"Max Absolute Error: {max_err:.6e}\")\n",
    "    print(f\"Mean Relative Error: {np.mean(rel_err):.6e}\")\n",
    "    print(f\"Max Relative Error: {np.max(rel_err):.6e}\")\n",
    "\n",
    "    # Optionally dump a few mismatches\n",
    "    if dump_mismatches:\n",
    "        mismatches = np.where(abs_err > tol)[0]\n",
    "        print(f\"\\nMismatches above tolerance {tol}: {len(mismatches)}\")\n",
    "        for idx in mismatches[:10]:  # show only first 10\n",
    "            print(f\"Idx {idx}: HW={hw[idx]}, SW={sw[idx]}, AbsErr={abs_err[idx]}\")\n",
    "\n",
    "    return mae, mse, max_err\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    hw_file = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\hw_output\\encoder0_out.txt\"\n",
    "    sw_file = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\software_output\\sw_encoder0_output.txt\"\n",
    "    compare_outputs(hw_file, sw_file, dump_mismatches=True, tol=1e-3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68acd65a-38cd-49b6-827c-dd93a6f4c4b1",
   "metadata": {},
   "source": [
    "### Datatype Conversion Understand:\n",
    "import numpy as np\n",
    "\n",
    "# ==== CONFIG ====\n",
    "input_txt = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_rgb_224.txt\"\n",
    "\n",
    "TOTAL_BITS = 16\n",
    "INT_BITS = 4\n",
    "FRAC_BITS = TOTAL_BITS - INT_BITS   # = 12\n",
    "SCALE = 2 ** FRAC_BITS\n",
    "MIN_VAL = - (2 ** (INT_BITS - 1))          # -8.0\n",
    "MAX_VAL = (2 ** (INT_BITS - 1)) - (1 / SCALE)  # 7.999755859375\n",
    "\n",
    "def float_to_ap_fixed(x):\n",
    "    \"\"\"Simulate ap_fixed<16,4>: quantize float to fixed-point representation.\"\"\"\n",
    "    # Saturate to representable range\n",
    "    x_clamped = np.clip(x, MIN_VAL, MAX_VAL)\n",
    "    # Scale to integer\n",
    "    fixed_val = np.round(x_clamped * SCALE).astype(np.int32)\n",
    "    return fixed_val\n",
    "\n",
    "def ap_fixed_to_float(fixed_val):\n",
    "    \"\"\"Convert back to float from fixed-point representation.\"\"\"\n",
    "    return fixed_val.astype(np.float32) / SCALE\n",
    "\n",
    "# ==== LOAD TXT ====\n",
    "with open(input_txt, \"r\") as f:\n",
    "    values = [float(v) for v in f.read().split()]\n",
    "\n",
    "values = np.array(values, dtype=np.float32)\n",
    "\n",
    "# ==== CONVERT TO FIXED & BACK ====\n",
    "fixed_vals = float_to_ap_fixed(values)\n",
    "roundtrip_vals = ap_fixed_to_float(fixed_vals)\n",
    "\n",
    "# ==== INSPECT ====\n",
    "for i in range(5):\n",
    "    print(f\"Original: {values[i]:.6f}, Fixed: {fixed_vals[i]}, Back-to-float: {roundtrip_vals[i]:.6f}\")\n",
    "\n",
    "# ==== OPTIONAL: error analysis ====\n",
    "abs_err = np.abs(values - roundtrip_vals)\n",
    "print(f\"\\nMax abs error: {abs_err.max():.6f}\")\n",
    "print(f\"Mean abs error: {abs_err.mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0048be85-b679-410f-8ef8-4946b6713c32",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "H, W, C = 224, 224, 3\n",
    "K, STRIDE, PAD = 3, 1, 1\n",
    "\n",
    "# Load file values into array\n",
    "with open(r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\\\img_rgb_224.txt\", \"r\") as f:\n",
    "    values = np.array([float(x) for x in f.read().split()], dtype=np.float32)\n",
    "\n",
    "# Reconstruct channel-major array\n",
    "img = np.zeros((H, W, C), dtype=np.float32)\n",
    "idx = 0\n",
    "for c in range(C):\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            img[i, j, c] = values[idx]\n",
    "            idx += 1\n",
    "\n",
    "# Map file index for each conv access\n",
    "def file_index(i, j, c):\n",
    "    return c * H * W + i * W + j\n",
    "\n",
    "# Example: first output pixel (oh=0, ow=0)\n",
    "iy, ix = 0, 0\n",
    "for ky in range(K):\n",
    "    for kx in range(K):\n",
    "        for ic in range(C):\n",
    "            val = img[iy+ky, ix+kx, ic]\n",
    "            idx = file_index(iy+ky, ix+kx, ic)\n",
    "            print(f\"a={val:.6f} comes from file index {idx}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2e865d4-a21a-4538-9a66-df33d573de8d",
   "metadata": {},
   "source": [
    "#2. Custom Stage0 software output mimic data_t\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "INT_BITS = 4\n",
    "FRAC_BITS = 12\n",
    "IMG_PATH = \"C:/Users/Baron/Desktop/EE_297_Repo/EE_297/hardware_imp/vitis_hls/lane_seg_hls/test_imgs/img_rgb_224.txt\"\n",
    "WEIGHT_SAVE_DIR = \"custom_dt_sw_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# FIXED-POINT QUANTIZATION\n",
    "# -------------------------------\n",
    "def quantize_fixed(x, int_bits=INT_BITS, frac_bits=FRAC_BITS):\n",
    "    scale = 2 ** frac_bits\n",
    "    max_val = (2 ** (int_bits - 1)) - 1 / scale\n",
    "    min_val = -(2 ** (int_bits - 1))\n",
    "    return np.clip(np.round(x * scale) / scale, min_val, max_val)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD IMAGE (.txt format, channel-major: R→G→B blocks)\n",
    "# -------------------------------\n",
    "def load_txt_tensor(filepath, H=224, W=224, C=3):\n",
    "    arr = np.loadtxt(filepath, dtype=np.float32)\n",
    "    if arr.size != C * H * W:\n",
    "        raise ValueError(f\"Unexpected image size: got {arr.size} elements\")\n",
    "    arr = arr.reshape(C, H, W)         # (C,H,W)\n",
    "    arr = np.expand_dims(arr, axis=0)  # (1,C,H,W)\n",
    "    return torch.tensor(arr, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# MODEL & BN FOLD (as before)\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device).eval()\n",
    "\n",
    "enc0 = model.encoder[0]\n",
    "conv: torch.nn.Conv2d = enc0[0]\n",
    "bn:   torch.nn.BatchNorm2d = enc0[1]\n",
    "\n",
    "# Fold BN into conv\n",
    "W = conv.weight.detach().cpu().numpy().astype(np.float32)         # (O,I,K,K)\n",
    "gamma = bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta  = bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean  = bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var   = bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps   = float(bn.eps)\n",
    "\n",
    "s = gamma / np.sqrt(var + eps)                                    # (O,)\n",
    "W_fold = W * s[:, None, None, None]                                # (O,I,K,K)\n",
    "b_fold = beta - mean * s                                           # (O,)\n",
    "\n",
    "# Quantize operands to ap_fixed grid\n",
    "Wq = quantize_fixed(W_fold)\n",
    "bq = quantize_fixed(b_fold)\n",
    "\n",
    "# -------------------------------\n",
    "# INPUT\n",
    "# -------------------------------\n",
    "x = load_txt_tensor(IMG_PATH).to(device)                           # (1,3,224,224)\n",
    "xq = quantize_fixed(x.cpu().numpy())                               # -> NumPy quantized\n",
    "\n",
    "# -------------------------------\n",
    "# Custom fixed-point conv2d (pure NumPy)\n",
    "# -------------------------------\n",
    "def fixed_conv2d_numpy(x_np, W_np, b_np, stride=2, pad=1, int_bits=INT_BITS, frac_bits=FRAC_BITS):\n",
    "    # x_np: (B,C_in,H,W), W_np: (C_out,C_in,kH,kW), b_np: (C_out,)\n",
    "    B, C_in, H, W_in = x_np.shape\n",
    "    C_out, _, kH, kW = W_np.shape\n",
    "    H_out = (H + 2*pad - kH)//stride + 1\n",
    "    W_out = (W_in + 2*pad - kW)//stride + 1\n",
    "    y = np.zeros((B, C_out, H_out, W_out), dtype=np.float32)\n",
    "\n",
    "    for n in range(B):\n",
    "        for oc in range(C_out):\n",
    "            for oh in range(H_out):\n",
    "                for ow in range(W_out):\n",
    "                    # start with quantized bias\n",
    "                    sum_q = quantize_fixed(b_np[oc], int_bits, frac_bits)\n",
    "                    iy0 = oh * stride - pad\n",
    "                    ix0 = ow * stride - pad\n",
    "                    for ic in range(C_in):\n",
    "                        for ky in range(kH):\n",
    "                            iy = iy0 + ky\n",
    "                            if iy < 0 or iy >= H: \n",
    "                                continue\n",
    "                            for kx in range(kW):\n",
    "                                ix = ix0 + kx\n",
    "                                if ix < 0 or ix >= W_in:\n",
    "                                    continue\n",
    "                                a = quantize_fixed(x_np[n, ic, iy, ix], int_bits, frac_bits)\n",
    "                                w = quantize_fixed(W_np[oc, ic, ky, kx], int_bits, frac_bits)\n",
    "                                p = quantize_fixed(a * w, int_bits, frac_bits)\n",
    "                                sum_q = quantize_fixed(sum_q + p, int_bits, frac_bits)\n",
    "                    # ReLU6 then final store quantization (matches data_t write)\n",
    "                    y[n, oc, oh, ow] = quantize_fixed(np.clip(sum_q, 0.0, 6.0), int_bits, frac_bits)\n",
    "    return y\n",
    "\n",
    "# Compute custom stage-0 output\n",
    "y_dt = fixed_conv2d_numpy(xq, Wq, bq, stride=2, pad=1)            # (1,32,112,112)\n",
    "\n",
    "# -------------------------------\n",
    "# SAVE\n",
    "# -------------------------------\n",
    "y_np = y_dt.squeeze(0).astype(np.float32)                          # (32,112,112)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder0_output_dt.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder0_output_dt.txt\"), \"w\") as f:\n",
    "    for c in range(y_np.shape[0]):\n",
    "        for i in range(y_np.shape[1]):\n",
    "            for j in range(y_np.shape[2]):\n",
    "                f.write(f\"{y_np[c, i, j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Custom fixed-point (ap_fixed<16,4>-style) stage0 output saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c576bd8-15ca-4576-a4ad-6aefb586f5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
