{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7450c338-5eaa-4ad8-99e8-3b642e487765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e119a4b-ec76-4a35-b92b-4b150c9a3360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a561caa1-c124-4143-82cc-e6078f52cb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205fbbb-9b09-4cbc-acf7-46920552871b",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad177fbc-b76b-4732-815e-81729dd76644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Necessary Liobraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "from tqdm.auto import tqdm as notebook_tqdm\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, AveragePooling2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#import torchvision.transforms as transforms\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fa35d-ef94-44b1-8bab-e4bb0ffd7d99",
   "metadata": {},
   "source": [
    "### RESIZING IMAGE (SW + HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc51230-f090-4c18-b5d2-a56b0210457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Input and output paths\n",
    "input_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og.jpg\"\n",
    "output_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(input_path)\n",
    "\n",
    "# Resize to 224x224\n",
    "resized_image = image.resize((224, 224))\n",
    "\n",
    "# Save the resized image\n",
    "resized_image.save(output_path)\n",
    "\n",
    "print(f\"Image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2efae-d8a8-4f97-8778-76fea27c721a",
   "metadata": {},
   "source": [
    "### CONVERTS IMAGE TO .TXT FILE (HW) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "55cefa19-5039-4f71-b19b-10e87b652a76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3def049c-fca4-4dd4-a235-39864a60dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Normalized image written to .txt for hardware input:\n",
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_rgb_224.txt\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== USER INPUT ====\n",
    "input_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\"\n",
    "output_basename = \"img_rgb_224.txt\"  # Output .txt filename\n",
    "target_size = (224, 224)             # Resize target\n",
    "\n",
    "# ==== LOAD IMAGE ====\n",
    "img = cv2.imread(input_path)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Could not load image at: {input_path}\")\n",
    "\n",
    "img = cv2.resize(img, target_size)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR → RGB\n",
    "img = img.astype(np.float32) / 255.0        # Normalize to [0,1]\n",
    "\n",
    "# ==== NORMALIZE WITH MEAN & STD (ImageNet-style) ====\n",
    "mean = np.array([0.485, 0.456, 0.406])  # R, G, B\n",
    "std  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "img = (img - mean[None, None, :]) / std[None, None, :]  # broadcast across H, W\n",
    "\n",
    "# ==== WRITE TO TXT ====\n",
    "output_dir = os.path.dirname(input_path)\n",
    "output_path = os.path.join(output_dir, output_basename)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    for c in range(3):  # R, G, B\n",
    "        for i in range(target_size[1]):\n",
    "            for j in range(target_size[0]):\n",
    "                f.write(f\"{img[i, j, c]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "print(f\"[✓] Normalized image written to .txt for hardware input:\\n{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09eebd-7a8b-4f26-9107-5328fd256dbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MODEL CREATION (SW)\n",
    "    1. LOAD IN TRAINED WEIGHTS FILE (SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8b318cd-955a-4e69-9431-4c4d5b4c1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class LaneSegOnlyMobileNetV2(nn.Module):\n",
    "    def __init__(self, pretrained=True, freeze_stem=False):\n",
    "        super().__init__()\n",
    "        # ---- Encoder: MobileNetV2 ----\n",
    "        weights = models.MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        m = models.mobilenet_v2(weights=weights)\n",
    "\n",
    "        # Feature extractor (ends at last conv-bn-relu block)\n",
    "        self.encoder = m.features              # output: (B, 1280, 7, 7) for 224x224\n",
    "        enc_out_ch = 1280\n",
    "\n",
    "        if freeze_stem:\n",
    "            # freeze early, cheap speed/regularization trick\n",
    "            for p in list(self.encoder.parameters())[:]:\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # ---- Lightweight decoder (no skip connections) ----\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(enc_out_ch, 256, kernel_size=2, stride=2),  # 7 -> 14\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),         # 14 -> 28\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),          # 28 -> 56\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),           # 56 -> 112\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 1, kernel_size=1),  # logits (B,1,112,112)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = x.shape[-2:]\n",
    "        feats = self.encoder(x)                 # (B,1280,7,7) at 224x224\n",
    "        seg   = self.seg_head(feats)           # (B,1,112,112)\n",
    "        seg   = F.interpolate(seg, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        return seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e33fb-c65f-45c6-8f03-1028412109ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bf193c-cd33-4cf9-b954-5aa2985c594b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_19856\\2694822331.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LaneSegOnlyMobileNetV2(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (seg_head): Sequential(\n",
       "    (0): ConvTranspose2d(1280, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LaneSegOnlyMobileNetV2()\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # set to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e58870c-37f4-48d9-9d03-af2bfbe36c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.0.weight torch.Size([32, 3, 3, 3])\n",
      "encoder.0.1.weight torch.Size([32])\n",
      "encoder.0.1.bias torch.Size([32])\n",
      "encoder.1.conv.0.0.weight torch.Size([32, 1, 3, 3])\n",
      "encoder.1.conv.0.1.weight torch.Size([32])\n",
      "encoder.1.conv.0.1.bias torch.Size([32])\n",
      "encoder.1.conv.1.weight torch.Size([16, 32, 1, 1])\n",
      "encoder.1.conv.2.weight torch.Size([16])\n",
      "encoder.1.conv.2.bias torch.Size([16])\n",
      "encoder.2.conv.0.0.weight torch.Size([96, 16, 1, 1])\n",
      "encoder.2.conv.0.1.weight torch.Size([96])\n",
      "encoder.2.conv.0.1.bias torch.Size([96])\n",
      "encoder.2.conv.1.0.weight torch.Size([96, 1, 3, 3])\n",
      "encoder.2.conv.1.1.weight torch.Size([96])\n",
      "encoder.2.conv.1.1.bias torch.Size([96])\n",
      "encoder.2.conv.2.weight torch.Size([24, 96, 1, 1])\n",
      "encoder.2.conv.3.weight torch.Size([24])\n",
      "encoder.2.conv.3.bias torch.Size([24])\n",
      "encoder.3.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "encoder.3.conv.0.1.weight torch.Size([144])\n",
      "encoder.3.conv.0.1.bias torch.Size([144])\n",
      "encoder.3.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "encoder.3.conv.1.1.weight torch.Size([144])\n",
      "encoder.3.conv.1.1.bias torch.Size([144])\n",
      "encoder.3.conv.2.weight torch.Size([24, 144, 1, 1])\n",
      "encoder.3.conv.3.weight torch.Size([24])\n",
      "encoder.3.conv.3.bias torch.Size([24])\n",
      "encoder.4.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "encoder.4.conv.0.1.weight torch.Size([144])\n",
      "encoder.4.conv.0.1.bias torch.Size([144])\n",
      "encoder.4.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "encoder.4.conv.1.1.weight torch.Size([144])\n",
      "encoder.4.conv.1.1.bias torch.Size([144])\n",
      "encoder.4.conv.2.weight torch.Size([32, 144, 1, 1])\n",
      "encoder.4.conv.3.weight torch.Size([32])\n",
      "encoder.4.conv.3.bias torch.Size([32])\n",
      "encoder.5.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.5.conv.0.1.weight torch.Size([192])\n",
      "encoder.5.conv.0.1.bias torch.Size([192])\n",
      "encoder.5.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.5.conv.1.1.weight torch.Size([192])\n",
      "encoder.5.conv.1.1.bias torch.Size([192])\n",
      "encoder.5.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "encoder.5.conv.3.weight torch.Size([32])\n",
      "encoder.5.conv.3.bias torch.Size([32])\n",
      "encoder.6.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.6.conv.0.1.weight torch.Size([192])\n",
      "encoder.6.conv.0.1.bias torch.Size([192])\n",
      "encoder.6.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.6.conv.1.1.weight torch.Size([192])\n",
      "encoder.6.conv.1.1.bias torch.Size([192])\n",
      "encoder.6.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "encoder.6.conv.3.weight torch.Size([32])\n",
      "encoder.6.conv.3.bias torch.Size([32])\n",
      "encoder.7.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.7.conv.0.1.weight torch.Size([192])\n",
      "encoder.7.conv.0.1.bias torch.Size([192])\n",
      "encoder.7.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.7.conv.1.1.weight torch.Size([192])\n",
      "encoder.7.conv.1.1.bias torch.Size([192])\n",
      "encoder.7.conv.2.weight torch.Size([64, 192, 1, 1])\n",
      "encoder.7.conv.3.weight torch.Size([64])\n",
      "encoder.7.conv.3.bias torch.Size([64])\n",
      "encoder.8.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.8.conv.0.1.weight torch.Size([384])\n",
      "encoder.8.conv.0.1.bias torch.Size([384])\n",
      "encoder.8.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.8.conv.1.1.weight torch.Size([384])\n",
      "encoder.8.conv.1.1.bias torch.Size([384])\n",
      "encoder.8.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.8.conv.3.weight torch.Size([64])\n",
      "encoder.8.conv.3.bias torch.Size([64])\n",
      "encoder.9.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.9.conv.0.1.weight torch.Size([384])\n",
      "encoder.9.conv.0.1.bias torch.Size([384])\n",
      "encoder.9.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.9.conv.1.1.weight torch.Size([384])\n",
      "encoder.9.conv.1.1.bias torch.Size([384])\n",
      "encoder.9.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.9.conv.3.weight torch.Size([64])\n",
      "encoder.9.conv.3.bias torch.Size([64])\n",
      "encoder.10.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.10.conv.0.1.weight torch.Size([384])\n",
      "encoder.10.conv.0.1.bias torch.Size([384])\n",
      "encoder.10.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.10.conv.1.1.weight torch.Size([384])\n",
      "encoder.10.conv.1.1.bias torch.Size([384])\n",
      "encoder.10.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.10.conv.3.weight torch.Size([64])\n",
      "encoder.10.conv.3.bias torch.Size([64])\n",
      "encoder.11.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.11.conv.0.1.weight torch.Size([384])\n",
      "encoder.11.conv.0.1.bias torch.Size([384])\n",
      "encoder.11.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.11.conv.1.1.weight torch.Size([384])\n",
      "encoder.11.conv.1.1.bias torch.Size([384])\n",
      "encoder.11.conv.2.weight torch.Size([96, 384, 1, 1])\n",
      "encoder.11.conv.3.weight torch.Size([96])\n",
      "encoder.11.conv.3.bias torch.Size([96])\n",
      "encoder.12.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.12.conv.0.1.weight torch.Size([576])\n",
      "encoder.12.conv.0.1.bias torch.Size([576])\n",
      "encoder.12.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.12.conv.1.1.weight torch.Size([576])\n",
      "encoder.12.conv.1.1.bias torch.Size([576])\n",
      "encoder.12.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "encoder.12.conv.3.weight torch.Size([96])\n",
      "encoder.12.conv.3.bias torch.Size([96])\n",
      "encoder.13.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.13.conv.0.1.weight torch.Size([576])\n",
      "encoder.13.conv.0.1.bias torch.Size([576])\n",
      "encoder.13.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.13.conv.1.1.weight torch.Size([576])\n",
      "encoder.13.conv.1.1.bias torch.Size([576])\n",
      "encoder.13.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "encoder.13.conv.3.weight torch.Size([96])\n",
      "encoder.13.conv.3.bias torch.Size([96])\n",
      "encoder.14.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.14.conv.0.1.weight torch.Size([576])\n",
      "encoder.14.conv.0.1.bias torch.Size([576])\n",
      "encoder.14.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.14.conv.1.1.weight torch.Size([576])\n",
      "encoder.14.conv.1.1.bias torch.Size([576])\n",
      "encoder.14.conv.2.weight torch.Size([160, 576, 1, 1])\n",
      "encoder.14.conv.3.weight torch.Size([160])\n",
      "encoder.14.conv.3.bias torch.Size([160])\n",
      "encoder.15.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.15.conv.0.1.weight torch.Size([960])\n",
      "encoder.15.conv.0.1.bias torch.Size([960])\n",
      "encoder.15.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.15.conv.1.1.weight torch.Size([960])\n",
      "encoder.15.conv.1.1.bias torch.Size([960])\n",
      "encoder.15.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "encoder.15.conv.3.weight torch.Size([160])\n",
      "encoder.15.conv.3.bias torch.Size([160])\n",
      "encoder.16.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.16.conv.0.1.weight torch.Size([960])\n",
      "encoder.16.conv.0.1.bias torch.Size([960])\n",
      "encoder.16.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.16.conv.1.1.weight torch.Size([960])\n",
      "encoder.16.conv.1.1.bias torch.Size([960])\n",
      "encoder.16.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "encoder.16.conv.3.weight torch.Size([160])\n",
      "encoder.16.conv.3.bias torch.Size([160])\n",
      "encoder.17.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.17.conv.0.1.weight torch.Size([960])\n",
      "encoder.17.conv.0.1.bias torch.Size([960])\n",
      "encoder.17.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.17.conv.1.1.weight torch.Size([960])\n",
      "encoder.17.conv.1.1.bias torch.Size([960])\n",
      "encoder.17.conv.2.weight torch.Size([320, 960, 1, 1])\n",
      "encoder.17.conv.3.weight torch.Size([320])\n",
      "encoder.17.conv.3.bias torch.Size([320])\n",
      "encoder.18.0.weight torch.Size([1280, 320, 1, 1])\n",
      "encoder.18.1.weight torch.Size([1280])\n",
      "encoder.18.1.bias torch.Size([1280])\n",
      "seg_head.0.weight torch.Size([1280, 256, 2, 2])\n",
      "seg_head.0.bias torch.Size([256])\n",
      "seg_head.1.weight torch.Size([256])\n",
      "seg_head.1.bias torch.Size([256])\n",
      "seg_head.3.weight torch.Size([256, 256, 3, 3])\n",
      "seg_head.3.bias torch.Size([256])\n",
      "seg_head.4.weight torch.Size([256])\n",
      "seg_head.4.bias torch.Size([256])\n",
      "seg_head.6.weight torch.Size([256, 128, 2, 2])\n",
      "seg_head.6.bias torch.Size([128])\n",
      "seg_head.7.weight torch.Size([128])\n",
      "seg_head.7.bias torch.Size([128])\n",
      "seg_head.9.weight torch.Size([128, 128, 3, 3])\n",
      "seg_head.9.bias torch.Size([128])\n",
      "seg_head.10.weight torch.Size([128])\n",
      "seg_head.10.bias torch.Size([128])\n",
      "seg_head.12.weight torch.Size([128, 64, 2, 2])\n",
      "seg_head.12.bias torch.Size([64])\n",
      "seg_head.13.weight torch.Size([64])\n",
      "seg_head.13.bias torch.Size([64])\n",
      "seg_head.15.weight torch.Size([64, 64, 3, 3])\n",
      "seg_head.15.bias torch.Size([64])\n",
      "seg_head.16.weight torch.Size([64])\n",
      "seg_head.16.bias torch.Size([64])\n",
      "seg_head.18.weight torch.Size([64, 32, 2, 2])\n",
      "seg_head.18.bias torch.Size([32])\n",
      "seg_head.19.weight torch.Size([32])\n",
      "seg_head.19.bias torch.Size([32])\n",
      "seg_head.21.weight torch.Size([32, 32, 3, 3])\n",
      "seg_head.21.bias torch.Size([32])\n",
      "seg_head.22.weight torch.Size([32])\n",
      "seg_head.22.bias torch.Size([32])\n",
      "seg_head.24.weight torch.Size([1, 32, 1, 1])\n",
      "seg_head.24.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a22b9-babe-40f0-9cd2-5dd5d464c93f",
   "metadata": {},
   "source": [
    "#### =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b21c8f-5207-43a6-9139-040d6ae84f44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (encoder): Sequential( HW Conv -> FOLD (BatchNorm) -> Relu)\n",
    "1. Save Weights File (HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cf1fbd-dea3-46c7-9f07-da325ec4d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv shape (O,I,K,K): (32, 3, 3, 3)\n",
      "BN shapes: (32,) (32,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#    It's a Conv2dNormActivation with (0)=Conv2d, (1)=BatchNorm2d, (2)=ReLU6\n",
    "# encoder[0] = Conv2dNormActivation: [0]=Conv2d(bias=False), [1]=BatchNorm2d, [2]=ReLU6\n",
    "enc0 = model.encoder[0]\n",
    "conv: nn.Conv2d = enc0[0]\n",
    "bn:   nn.BatchNorm2d = enc0[1]\n",
    "\n",
    "assert isinstance(conv, nn.Conv2d)\n",
    "assert isinstance(bn,   nn.BatchNorm2d)\n",
    "\n",
    "print(\"Conv shape (O,I,K,K):\", tuple(conv.weight.shape))\n",
    "print(\"BN shapes:\", tuple(bn.weight.shape), tuple(bn.bias.shape))\n",
    "\n",
    "# Setting up BatchNorm Math Fold:\n",
    "# Do folding in float32, cast once at the end\n",
    "W32    = conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma32= bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta32 = bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean32 = bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var32  = bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps32  = np.float32(bn.eps)\n",
    "\n",
    "s32    = gamma32 / np.sqrt(var32 + eps32)\n",
    "W_fold = (W32 * s32[:, None, None, None]).astype(np.float16)\n",
    "b_fold = (beta32 - mean32 * s32).astype(np.float16)\n",
    "\n",
    "# If your HLS expects (K,K,IN_C,OUT_C) layout:\n",
    "W_hls = np.transpose(W_fold, (2, 3, 1, 0))  # (K,K,I,O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "121a2fbb-db90-48d8-a435-29d183295eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\encoder_conv0_w.h\n",
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\encoder_conv0_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"      # relative path in your project\n",
    "CTYPE   = \"data_t\"       # ap_fixed typedef in your HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Writes a header with:\n",
    "      - #pragma once\n",
    "      - your include path\n",
    "      - declaration: data_t var[... ] = { ... };\n",
    "      - numbers printed as plain literals (no 'f', no '(data_t)' casts)\n",
    "    \"\"\"\n",
    "    array = np.asarray(array, dtype=np.float16)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            # no 'f' and no casts; compact scientific where needed\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f'{HEADER_INCLUDE}\\n\\n')\n",
    "        f.write(\"// Auto-generated from encoder[0] with BN folded\\n\\n\")\n",
    "        # no 'const' here\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    # print full path (one-liner)\n",
    "    print(os.path.abspath(out_path))\n",
    "\n",
    "# ---- call these with your already-computed arrays ----\n",
    "# W_hls: (K,K,IN_C,OUT_C), b_fold: (OUT_C,)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "write_header_plain_numbers(W_hls, \"conv0_w\", CTYPE, os.path.join(OUT_DIR, \"encoder_conv0_w.h\"))\n",
    "write_header_plain_numbers(b_fold, \"conv0_b\", CTYPE, os.path.join(OUT_DIR, \"encoder_conv0_b.h\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c4cd2-9fea-43c0-a8c5-4675c0ae4b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee8292cf-76d1-4776-92e2-a9e3b13b949f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (encoder): SW output of Conv Stage 0 Comparison\n",
    "     1. Actual Model Software Output\n",
    "     2. Custom Stage0 software output mimic data_t\n",
    "     3. Print Debug Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4248ee22-2ad4-4a8b-8e2b-4b0ea9f974a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\636227901.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_q dtype: torch.float16\n",
      "W_torch dtype: torch.float16\n",
      "b_torch dtype: torch.float16\n",
      "conv2d output dtype: torch.float16\n",
      "final output dtype: torch.float16\n",
      "✅ Software output with FP16 weights, input, and arithmetic saved.\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "IMG_PATH = \"C:/Users/Baron/Desktop/EE_297_Repo/EE_297/hardware_imp/vitis_hls/lane_seg_hls/test_imgs/img_rgb_224.txt\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD IMAGE (.txt format, channel-major: R→G→B blocks)\n",
    "# -------------------------------\n",
    "def load_txt_tensor(filepath, H=224, W=224, C=3):\n",
    "    arr = np.loadtxt(filepath, dtype=np.float16)\n",
    "    if arr.size != C * H * W:\n",
    "        raise ValueError(f\"Unexpected image size: got {arr.size} elements\")\n",
    "\n",
    "    arr = arr.reshape(C, H, W)          # (C,H,W), channel-major\n",
    "    arr = np.expand_dims(arr, axis=0)   # (1,C,H,W) for torch\n",
    "    return torch.tensor(arr, dtype=torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# MAIN PIPELINE\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[0] weights and BN params\n",
    "enc0 = model.encoder[0]\n",
    "conv: torch.nn.Conv2d = enc0[0]\n",
    "bn:   torch.nn.BatchNorm2d = enc0[1]\n",
    "\n",
    "# Do folding in float32, cast once at the end\n",
    "W32    = conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma32= bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta32 = bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean32 = bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var32  = bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps32  = np.float32(bn.eps)\n",
    "\n",
    "#======================== After this point we need float16 ============\n",
    "s32     = gamma32 / np.sqrt(var32 + eps32)\n",
    "W_fold  = (W32 * s32[:, None, None, None]).astype(np.float16)\n",
    "b_fold  = (beta32 - mean32 * s32).astype(np.float16)\n",
    "\n",
    "W_torch = torch.tensor(W_fold, dtype=torch.float16, device=device)\n",
    "b_torch = torch.tensor(b_fold, dtype=torch.float16, device=device)\n",
    "\n",
    "# Load image directly from .txt\n",
    "x_q = load_txt_tensor(IMG_PATH).to(device)\n",
    "\n",
    "print(\"x_q dtype:\", x_q.dtype)\n",
    "print(\"W_torch dtype:\", W_torch.dtype)\n",
    "print(\"b_torch dtype:\", b_torch.dtype)\n",
    "\n",
    "# -------------------------------\n",
    "# Forward block (FP16)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Normal float16 conv\n",
    "    y = F.conv2d(x_q, W_torch, bias=b_torch, stride=2, padding=1)\n",
    "    print(\"conv2d output dtype:\", y.dtype)\n",
    "    \n",
    "    # Apply ReLU6 and clamp\n",
    "    y = torch.clamp(y, 0, 6)\n",
    "    print(\"final output dtype:\", y.dtype)\n",
    "    \n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder0_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder0_output.txt\"), \"w\") as f:\n",
    "    for c in range(32):\n",
    "        for i in range(112):\n",
    "            for j in range(112):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output with FP16 weights, input, and arithmetic saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c506b-9461-49a3-9d5b-cfc6a0edee72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb7e25c-6b68-487d-b195-50f34e195a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: bias[0] = -0.022598\n",
      "DEBUG: weights[0][0][0][0] = 0.006657\n",
      "DEBUG: a=0.000000 * b=0.006657 => 0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=0.003578 => 0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=-0.016006 => -0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=-0.003366 => -0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=-0.004288 => -0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=-0.012825 => -0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=0.007576 => 0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=0.007786 => 0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=-0.006905 => -0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=0.018478 => 0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=0.010941 => 0.000000 | sum=-0.022598\n",
      "DEBUG: a=0.000000 * b=-0.007534 => -0.000000 | sum=-0.022598\n",
      "DEBUG: a=-1.500977 * b=-0.015869 => 0.023819 | sum=0.001221\n",
      "DEBUG: a=-1.265625 * b=-0.020370 => 0.025787 | sum=0.027008\n",
      "DEBUG: a=-1.002930 * b=-0.018280 => 0.018341 | sum=0.045349\n",
      "DEBUG: a=-1.518555 * b=0.003067 => -0.004658 | sum=0.040680\n",
      "DEBUG: a=-1.283203 * b=-0.002193 => 0.002815 | sum=0.043488\n",
      "DEBUG: a=-1.020508 * b=-0.009903 => 0.010109 | sum=0.053589\n",
      "DEBUG: a=0.000000 * b=0.005299 => 0.000000 | sum=0.053589\n",
      "DEBUG: a=0.000000 * b=0.005882 => 0.000000 | sum=0.053589\n",
      "DEBUG: a=0.000000 * b=-0.016617 => -0.000000 | sum=0.053589\n",
      "DEBUG: a=-1.689453 * b=-0.023071 => 0.038971 | sum=0.092529\n",
      "DEBUG: a=-1.458008 * b=-0.020477 => 0.029861 | sum=0.122375\n",
      "DEBUG: a=-1.194336 * b=-0.020782 => 0.024826 | sum=0.147217\n",
      "DEBUG: a=-1.672852 * b=-0.009621 => 0.016098 | sum=0.163330\n",
      "DEBUG: a=-1.440430 * b=-0.005470 => 0.007881 | sum=0.171265\n",
      "DEBUG: a=-1.176758 * b=-0.015213 => 0.017899 | sum=0.189209\n",
      "DEBUG: output[0][0][0] = 0.189209\n"
     ]
    }
   ],
   "source": [
    "#3. Print Debug Statements\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------------------\n",
    "# Step 1: Create HLS-style padded buffer\n",
    "# -----------------------------------------\n",
    "def pad_like_hls_layout(x, pad=1):\n",
    "    \"\"\"\n",
    "    Mimics HLS padded[y+PAD][x+PAD][c] = input[y][x][c]\n",
    "    Input:  x (1,C,H,W)\n",
    "    Output: (H+2*pad, W+2*pad, C) with channels innermost\n",
    "    \"\"\"\n",
    "    _, C, H, W = x.shape\n",
    "    out = np.zeros((H + 2*pad, W + 2*pad, C), dtype=np.float16)\n",
    "    for c in range(C):\n",
    "        out[pad:pad+H, pad:pad+W, c] = x[0, c, :, :].cpu().numpy()\n",
    "    return out\n",
    "\n",
    "PAD = 1\n",
    "x_padded_hls = pad_like_hls_layout(x_q, pad=PAD)   # shape (226,226,3)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Step 2: Debug the very first conv output: [oh=0, ow=0, oc=0]\n",
    "# -----------------------------------------\n",
    "STRIDE = 2\n",
    "K = 3\n",
    "iy = 0 * STRIDE\n",
    "ix = 0 * STRIDE\n",
    "oc = 0\n",
    "\n",
    "# Bias (fp16)\n",
    "bias_debug = np.float16(b_torch[oc].item())\n",
    "print(f\"DEBUG: bias[0] = {bias_debug:.6f}\")\n",
    "\n",
    "# First weight (fp16)\n",
    "w_debug = np.float16(W_torch[oc, 0, 0, 0].item())\n",
    "print(f\"DEBUG: weights[0][0][0][0] = {w_debug:.6f}\")\n",
    "\n",
    "sum_debug = bias_debug\n",
    "\n",
    "for ky in range(K):\n",
    "    for kx in range(K):\n",
    "        for ic in range(3):  # IN_C = 3\n",
    "            # Input + weight (fp16)\n",
    "            a = np.float16(x_padded_hls[iy + ky, ix + kx, ic])\n",
    "            b = np.float16(W_torch[oc, ic, ky, kx].item())\n",
    "            # Multiply and accumulate (fp16)\n",
    "            p = np.float16(a * b)\n",
    "            sum_debug = np.float16(sum_debug + p)\n",
    "            print(f\"DEBUG: a={a:.6f} * b={b:.6f} => {p:.6f} | sum={sum_debug:.6f}\")\n",
    "\n",
    "# Final ReLU6 (fp16)\n",
    "sum_clamped = np.float16(max(0, min(6, sum_debug)))\n",
    "print(f\"DEBUG: output[0][0][0] = {sum_clamped:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce1ed796-e546-4797-aaa7-9028434faf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Quantization Error Report ====\n",
      "Number of elements: 401408\n",
      "Mean Absolute Error (MAE): 5.512016e-04\n",
      "Mean Squared Error (MSE): 1.482759e-06\n",
      "Max Absolute Error: 1.953200e-02\n",
      "Mean Relative Error: 5.433126e+04\n",
      "Max Relative Error: 4.700000e+09\n",
      "\n",
      "Mismatches above tolerance 0.01: 223\n",
      "Idx 12613: HW=0.520996, SW=0.532227, AbsErr=0.011230999999999991\n",
      "Idx 12742: HW=0.563965, SW=0.576172, AbsErr=0.012206999999999968\n",
      "Idx 12951: HW=0.631836, SW=0.643066, AbsErr=0.011230000000000073\n",
      "Idx 12970: HW=0.595703, SW=0.606445, AbsErr=0.01074200000000003\n",
      "Idx 13064: HW=0.649902, SW=0.660156, AbsErr=0.010253999999999985\n",
      "Idx 13248: HW=0.771973, SW=0.78418, AbsErr=0.012206999999999968\n",
      "Idx 13364: HW=0.595703, SW=0.606934, AbsErr=0.011230999999999991\n",
      "Idx 13370: HW=0.663086, SW=0.67334, AbsErr=0.010254000000000096\n",
      "Idx 13392: HW=0.627441, SW=0.637695, AbsErr=0.010253999999999985\n",
      "Idx 13399: HW=0.635742, SW=0.648438, AbsErr=0.01269599999999993\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_txt_file(path):\n",
    "    \"\"\"\n",
    "    Load numbers from a text file.\n",
    "    Handles space, comma, or newline separated values.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    # Split by whitespace or commas\n",
    "    numbers = text.replace(\",\", \" \").split()\n",
    "    return np.array([float(x) for x in numbers], dtype=np.float64)\n",
    "\n",
    "def compare_outputs(hw_file, sw_file, dump_mismatches=False, tol=1e-5):\n",
    "    # Load both files\n",
    "    hw = load_txt_file(hw_file)\n",
    "    sw = load_txt_file(sw_file)\n",
    "\n",
    "    if hw.shape != sw.shape:\n",
    "        raise ValueError(f\"Shape mismatch: HW={hw.shape}, SW={sw.shape}\")\n",
    "\n",
    "    # Compute error metrics\n",
    "    abs_err = np.abs(hw - sw)\n",
    "    rel_err = abs_err / (np.abs(sw) + 1e-12)  # avoid div by zero\n",
    "\n",
    "    mse = np.mean(abs_err**2)\n",
    "    mae = np.mean(abs_err)\n",
    "    max_err = np.max(abs_err)\n",
    "\n",
    "    print(\"==== Quantization Error Report ====\")\n",
    "    print(f\"Number of elements: {hw.size}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.6e}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.6e}\")\n",
    "    print(f\"Max Absolute Error: {max_err:.6e}\")\n",
    "    print(f\"Mean Relative Error: {np.mean(rel_err):.6e}\")\n",
    "    print(f\"Max Relative Error: {np.max(rel_err):.6e}\")\n",
    "\n",
    "    # Optionally dump a few mismatches\n",
    "    if dump_mismatches:\n",
    "        mismatches = np.where(abs_err > tol)[0]\n",
    "        print(f\"\\nMismatches above tolerance {tol}: {len(mismatches)}\")\n",
    "        for idx in mismatches[:10]:  # show only first 10\n",
    "            print(f\"Idx {idx}: HW={hw[idx]}, SW={sw[idx]}, AbsErr={abs_err[idx]}\")\n",
    "\n",
    "    return mae, mse, max_err\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    hw_file = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\hw_output\\encoder0_out.txt\"\n",
    "    sw_file = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\software_output\\sw_encoder0_output.txt\"\n",
    "    compare_outputs(hw_file, sw_file, dump_mismatches=True, tol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f9059-ce3a-43dd-8ac6-f34b7df04fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51bdbd-3744-4abe-9572-ac5d82c24e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea94370-686c-47e1-86ee-e7cb978fcd8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual0): HW output of Conv Stage 1 \n",
    "         1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b36e63-b447-4395-a3a9-e2d5bb184322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depthwise conv shape (O,I,K,K): (32, 1, 3, 3)\n",
      "Depthwise BN shapes: (32,) (32,)\n",
      "Pointwise conv shape (O,I,K,K): (16, 32, 1, 1)\n",
      "Pointwise BN shapes: (16,) (16,)\n"
     ]
    }
   ],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 1)\n",
    "ir0 = model.encoder[1]        # InvertedResidual\n",
    "convseq = ir0.conv            # Sequential inside it\n",
    "\n",
    "# ---- Depthwise ConvNormActivation ----\n",
    "dw_conv: nn.Conv2d = convseq[0][0]   # Conv2d depthwise\n",
    "dw_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "assert isinstance(dw_conv, nn.Conv2d)\n",
    "assert isinstance(dw_bn,   nn.BatchNorm2d)\n",
    "\n",
    "print(\"Depthwise conv shape (O,I,K,K):\", tuple(dw_conv.weight.shape))\n",
    "print(\"Depthwise BN shapes:\", tuple(dw_bn.weight.shape), tuple(dw_bn.bias.shape))\n",
    "\n",
    "# Extract tensors\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")   # (O,I,K,K)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "# Fold BN into depthwise conv\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C)\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Pointwise Conv + BN ----\n",
    "pw_conv: nn.Conv2d = convseq[1]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[2]\n",
    "\n",
    "assert isinstance(pw_conv, nn.Conv2d)\n",
    "assert isinstance(pw_bn,   nn.BatchNorm2d)\n",
    "\n",
    "print(\"Pointwise conv shape (O,I,K,K):\", tuple(pw_conv.weight.shape))\n",
    "print(\"Pointwise BN shapes:\", tuple(pw_bn.weight.shape), tuple(pw_bn.bias.shape))\n",
    "\n",
    "# Extract tensors\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")   # (O,I,K,K)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "# Fold BN into pointwise conv\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — here K=1\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c9b1844-264a-4171-9ac2-c9084f26104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc1_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Depthwise\n",
    "write_header_plain_numbers(W_dw_hls, \"enc1_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc1_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc1_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc1_dw_b.h\"))\n",
    "\n",
    "# Pointwise\n",
    "write_header_plain_numbers(W_pw_hls, \"enc1_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc1_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc1_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc1_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220980e7-c5de-4765-ab26-7412358421d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual0): SW output of Conv Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953df13-36ff-43e8-b617-6bdb08cb5c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82fb89ca-2a61-40e8-8636-eb6d539b6164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\854751219.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d output dtype: torch.float16\n",
      "conv2d output dtype: torch.float16\n",
      "✅ Software output for enc1_ir0 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE0_OUT_PATH = \"software_output/sw_encoder0_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE0_OUT_PATH).astype(np.float32)   # shape (32,112,112)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,32,112,112)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[1] (InvertedResidual)\n",
    "ir0 = model.encoder[1]\n",
    "convseq = ir0.conv\n",
    "\n",
    "# ---- Depthwise Conv (with BN folded) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (32,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "#======================== After this point we need float16 ============\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = W_dw * s_dw[:, None, None, None].astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Pointwise Conv (with BN folded) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[1]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[2]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (16,32,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "#======================== After this point we need float16 ============\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = W_pw * s_pw[:, None, None, None].astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_dw_torch = torch.tensor(W_dw_fold, dtype=torch.float16, device=device)\n",
    "b_dw_torch = torch.tensor(b_dw_fold, dtype=torch.float16, device=device)\n",
    "W_pw_torch = torch.tensor(W_pw_fold, dtype=torch.float16, device=device)\n",
    "b_pw_torch = torch.tensor(b_pw_fold, dtype=torch.float16, device=device)\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 1 (depthwise + pointwise)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    y = F.conv2d(x_torch, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=32)\n",
    "    print(\"conv2d output dtype:\", y.dtype)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6 after depthwise\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    print(\"conv2d output dtype:\", y.dtype)\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (16,112,112)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder1_ir0_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder1_ir0_output.txt\"), \"w\") as f:\n",
    "    for c in range(16):\n",
    "        for i in range(112):\n",
    "            for j in range(112):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc1_ir0 saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ca1b9e-a305-40ab-b805-32d666f2571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG DW: bias[0] = -0.014267\n",
      "DW DEBUG: a=0.189087 * b=0.028519 => 0.005394 | sum=-0.008873\n",
      "DW DEBUG: a=0.154785 * b=0.760254 => 0.117676 | sum=0.108826\n",
      "DW DEBUG: a=0.227295 * b=-0.062988 => -0.014320 | sum=0.094482\n",
      "DW DEBUG: a=0.180054 * b=0.057465 => 0.010345 | sum=0.104858\n",
      "DW DEBUG: output[0][0][0] after ReLU6 = 0.104858\n",
      "PW DEBUG: bias[0] = -2.091797\n",
      "PW DEBUG: ic=0, a=0.104858, b=-0.010025, p=-0.001051, sum=-2.093750\n",
      "PW DEBUG: ic=1, a=1.621094, b=0.081299, p=0.131836, sum=-1.961914\n",
      "PW DEBUG: ic=2, a=2.710938, b=-0.066345, p=-0.179810, sum=-2.142578\n",
      "PW DEBUG: output[0][0][0] final = -1.787109\n"
     ]
    }
   ],
   "source": [
    "# === enc1_ir0 single-pixel debug (oh=0, ow=0), HLS-faithful ===\n",
    "import numpy as np\n",
    "\n",
    "# Pull raw arrays (keep dtype consistent with your forward pass: float16 or float32)\n",
    "x_np  = x_torch.squeeze(0).detach().cpu().numpy()            # (32,112,112)\n",
    "Wdw   = W_dw_torch.detach().cpu().numpy()                    # (32,1,3,3)  -> use [oc,0,ky,kx]\n",
    "bdw   = b_dw_torch.detach().cpu().numpy()                    # (32,)\n",
    "Wpw   = W_pw_torch.detach().cpu().numpy()                    # (16,32,1,1) -> use [oc,ic,0,0]\n",
    "bpw   = b_pw_torch.detach().cpu().numpy()                    # (16,)\n",
    "\n",
    "OUT_H = x_np.shape[1]    # 112\n",
    "OUT_W = x_np.shape[2]    # 112\n",
    "K = 3\n",
    "oh = ow = 0\n",
    "\n",
    "# ---- Depthwise: compute dw_out for ALL channels at (0,0) ----\n",
    "dw_out00 = np.zeros((x_np.shape[0],), dtype=x_np.dtype)      # (32,)\n",
    "for c in range(x_np.shape[0]):\n",
    "    s = bdw[c]\n",
    "    # DEBUG: channel 0 prints like HLS\n",
    "    if c == 0:\n",
    "        print(f\"DEBUG DW: bias[{c}] = {float(bdw[c]):.6f}\")\n",
    "    for ky in range(K):\n",
    "        for kx in range(K):\n",
    "            iy = oh + ky - 1  # PAD=1 via bounds check\n",
    "            ix = ow + kx - 1\n",
    "            if 0 <= iy < OUT_H and 0 <= ix < OUT_W:\n",
    "                a = x_np[c, iy, ix]\n",
    "                b = Wdw[c, 0, ky, kx]    # depthwise weight for this channel\n",
    "                p = a * b\n",
    "                s = s + p\n",
    "                if c == 0:\n",
    "                    print(f\"DW DEBUG: a={float(a):.6f} * b={float(b):.6f} => {float(p):.6f} | sum={float(s):.6f}\")\n",
    "    # ReLU6\n",
    "    if s < 0:   s = type(s)(0)\n",
    "    elif s > 6: s = type(s)(6)\n",
    "    dw_out00[c] = s\n",
    "    if c == 0:\n",
    "        print(f\"DW DEBUG: output[{oh}][{ow}][{c}] after ReLU6 = {float(s):.6f}\")\n",
    "\n",
    "# ---- Pointwise: oc=0 at same pixel, accumulate across ALL ic ----\n",
    "oc_pw = 0\n",
    "sum_pw = bpw[oc_pw]\n",
    "print(f\"PW DEBUG: bias[{oc_pw}] = {float(sum_pw):.6f}\")\n",
    "for ic in range(x_np.shape[0]):          # 32 channels\n",
    "    a = dw_out00[ic]\n",
    "    b = Wpw[oc_pw, ic, 0, 0]\n",
    "    p = a * b\n",
    "    sum_pw = sum_pw + p\n",
    "    if ic < 3:  # match your HLS printf subset\n",
    "        print(f\"PW DEBUG: ic={ic}, a={float(a):.6f}, b={float(b):.6f}, p={float(p):.6f}, sum={float(sum_pw):.6f}\")\n",
    "\n",
    "print(f\"PW DEBUG: output[{oh}][{ow}][{oc_pw}] final = {float(sum_pw):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c2c5f1-6af3-4711-b25f-fc7ff7c36e35",
   "metadata": {},
   "source": [
    "sw = np.loadtxt(r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\software_output\\sw_encoder1_ir0_output.txt\")\n",
    "hw = np.loadtxt(r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\hw_output\\enc0_ir0_out.txt\")\n",
    "\n",
    "print(sw.shape, hw.shape)\n",
    "sw = sw.reshape(16, 112, 112)\n",
    "hw = hw.reshape(16, 112, 112)\n",
    "\n",
    "print(np.allclose(sw[0], hw[0], atol=1e-3))  # compare just channel 0\n",
    "print(np.max(np.abs(sw - hw)))               # worst-case diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eece12b-20b0-4d9e-941e-48a90f695d0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual1): HW output of Conv Stage 2\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1bf0f56-2aa1-4a35-97ec-1d67aa7e2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 2 for enc2_ir1)\n",
    "ir1 = model.encoder[2]        # InvertedResidual (your \"enc2_ir1\")\n",
    "convseq = ir1.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 16→96) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (96,16,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,16,96)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=96) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (96,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=96, so IN=1 per channel\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,96)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 96→24) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (24,96,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,96,24)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ab446a7-14df-48f8-ba07-31c2bcf404c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc2_ir1_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc2_ir1_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc2_ir1_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc2_ir1_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc2_ir1_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc2_ir1_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 16→96)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc2_ir1_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc2_ir1_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc2_ir1_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc2_ir1_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=2, groups=96)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc2_ir1_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc2_ir1_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc2_ir1_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc2_ir1_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 96→24)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc2_ir1_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc2_ir1_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc2_ir1_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc2_ir1_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323ad7d-8061-4b0f-9582-1bdc32a4ee6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual1): SW output of Conv Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eca85a13-04a0-4d1d-aecf-ab0ba638a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\4104754890.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc2_ir1 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE1_OUT_PATH = \"software_output/sw_encoder1_ir0_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE1_OUT_PATH).astype(np.float32)   # shape (16,112,112)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,16,112,112)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[2] (InvertedResidual)\n",
    "ir1 = model.encoder[2]\n",
    "convseq = ir1.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 16→96) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (96,16,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=96) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (96,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 96→24) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (24,96,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 2 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=2, padding=1, groups=96)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (24,56,56)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder2_ir1_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder2_ir1_output.txt\"), \"w\") as f:\n",
    "    for c in range(24):\n",
    "        for i in range(56):\n",
    "            for j in range(56):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc2_ir1 saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0d5df8d-4856-42f1-9c25-5cf246aae426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP DEBUG: output[0][0][0] = 1.440430\n",
      "DW DEBUG: bias[0] = 0.371826\n",
      "DW DEBUG: a=1.440430 * b=-0.578125 => -0.832748 | sum=-0.460922\n",
      "DW DEBUG: a=1.109375 * b=-0.298340 => -0.330971 | sum=-0.791893\n",
      "DW DEBUG: a=0.956055 * b=-0.311035 => -0.297367 | sum=-1.089260\n",
      "DW DEBUG: a=0.686523 * b=-0.141846 => -0.097380 | sum=-1.186640\n",
      "DW DEBUG: output[0][0][0] after ReLU6 = 0.000000\n",
      "PW DEBUG: bias[0] = -1.168945\n",
      "PW DEBUG: ic=0, a=0.000000, b=0.259033, p=0.000000, sum=-1.168945\n",
      "PW DEBUG: ic=1, a=0.386230, b=0.259033, p=0.100047, sum=-1.068899\n",
      "PW DEBUG: ic=2, a=0.000000, b=-0.005741, p=-0.000000, sum=-1.068899\n",
      "PW DEBUG: output[0][0][0] final = 0.302640\n"
     ]
    }
   ],
   "source": [
    "# === enc2_ir1 single-pixel debug (oy=0, ox=0), HLS-faithful ===\n",
    "import numpy as np\n",
    "\n",
    "# Pull raw arrays\n",
    "x_np  = x_torch.squeeze(0).detach().cpu().numpy()            # (16,112,112)\n",
    "Wexp  = W_exp_torch.detach().cpu().numpy()                   # (96,16,1,1)\n",
    "bexp  = b_exp_torch.detach().cpu().numpy()                   # (96,)\n",
    "Wdw   = W_dw_torch.detach().cpu().numpy()                    # (96,1,3,3)\n",
    "bdw   = b_dw_torch.detach().cpu().numpy()                    # (96,)\n",
    "Wpw   = W_pw_torch.detach().cpu().numpy()                    # (24,96,1,1)\n",
    "bpw   = b_pw_torch.detach().cpu().numpy()                    # (24,)\n",
    "\n",
    "IN_C   = x_np.shape[0]   # 16\n",
    "IN_H   = x_np.shape[1]   # 112\n",
    "IN_W   = x_np.shape[2]   # 112\n",
    "EXP_C  = Wexp.shape[0]   # 96\n",
    "OUT_C  = Wpw.shape[0]    # 24\n",
    "K = 3\n",
    "\n",
    "oy = ox = 0\n",
    "iy0 = oy * 2\n",
    "ix0 = ox * 2\n",
    "\n",
    "# ---- Expansion: compute exp_out at *all positions* (not just (0,0)) ----\n",
    "exp_out = np.zeros((EXP_C, IN_H, IN_W), dtype=np.float32)\n",
    "for oc in range(EXP_C):\n",
    "    for iy in range(IN_H):\n",
    "        for ix in range(IN_W):\n",
    "            s = bexp[oc]\n",
    "            for ic in range(IN_C):\n",
    "                s += x_np[ic, iy, ix] * Wexp[oc, ic, 0, 0]\n",
    "            # ReLU6\n",
    "            s = np.clip(s, 0, 6)\n",
    "            exp_out[oc, iy, ix] = s\n",
    "\n",
    "# Debug one pixel/channel\n",
    "print(f\"EXP DEBUG: output[0][0][0] = {float(exp_out[0,0,0]):.6f}\")\n",
    "\n",
    "# ---- Depthwise: use proper 3x3 neighborhood of expanded feature map ----\n",
    "dw_out00 = np.zeros((EXP_C,), dtype=np.float32)\n",
    "for c in range(EXP_C):\n",
    "    s = bdw[c]\n",
    "    if c == 0:\n",
    "        print(f\"DW DEBUG: bias[{c}] = {float(s):.6f}\")\n",
    "    for ky in range(K):\n",
    "        for kx in range(K):\n",
    "            iy = iy0 + ky - 1\n",
    "            ix = ix0 + kx - 1\n",
    "            if 0 <= iy < IN_H and 0 <= ix < IN_W:\n",
    "                a = exp_out[c, iy, ix]\n",
    "                b = Wdw[c, 0, ky, kx]\n",
    "                p = a * b\n",
    "                s += p\n",
    "                if c == 0:\n",
    "                    print(f\"DW DEBUG: a={float(a):.6f} * b={float(b):.6f} => {float(p):.6f} | sum={float(s):.6f}\")\n",
    "    # ReLU6\n",
    "    s = np.clip(s, 0, 6)\n",
    "    dw_out00[c] = s\n",
    "    if c == 0:\n",
    "        print(f\"DW DEBUG: output[{oy}][{ox}][{c}] after ReLU6 = {float(s):.6f}\")\n",
    "\n",
    "# ---- Projection: oc=0 at same pixel ----\n",
    "oc_pw = 0\n",
    "sum_pw = bpw[oc_pw]\n",
    "print(f\"PW DEBUG: bias[{oc_pw}] = {float(sum_pw):.6f}\")\n",
    "for ic in range(EXP_C):\n",
    "    a = dw_out00[ic]\n",
    "    b = Wpw[oc_pw, ic, 0, 0]\n",
    "    p = a * b\n",
    "    sum_pw += p\n",
    "    if ic < 3:\n",
    "        print(f\"PW DEBUG: ic={ic}, a={float(a):.6f}, b={float(b):.6f}, p={float(p):.6f}, sum={float(sum_pw):.6f}\")\n",
    "\n",
    "print(f\"PW DEBUG: output[{oy}][{ox}][{oc_pw}] final = {float(sum_pw):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c5263-b0ab-41b0-bc63-161fb07845ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual2): HW output of Conv Stage 3\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70669a76-0e5f-491b-b80e-a8c0df31dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 3 for enc3_ir2)\n",
    "ir2 = model.encoder[3]        # InvertedResidual (your \"enc3_ir2\")\n",
    "convseq = ir2.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 24→144) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (144,24,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,24,144)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=144) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (144,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=144, so IN=1 per channel\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,144)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 144→24) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (24,144,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,144,24)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b674278b-7414-4864-8628-6de8e6ce85fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc3_ir2_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc3_ir2_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc3_ir2_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc3_ir2_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc3_ir2_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc3_ir2_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 24→144)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc3_ir2_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc3_ir2_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc3_ir2_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc3_ir2_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=144)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc3_ir2_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc3_ir2_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc3_ir2_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc3_ir2_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 144→24)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc3_ir2_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc3_ir2_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc3_ir2_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc3_ir2_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f4f33-fa9f-4240-887c-a5e0dcf6a29d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual2): SW output of Conv Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d82ca0e-8087-47a3-b7c3-e3629e705780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\454475225.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc3_ir2 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE2_OUT_PATH = \"software_output/sw_encoder2_ir1_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE2_OUT_PATH).astype(np.float32)   # shape (24,56,56)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,24,56,56)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[3] (InvertedResidual)\n",
    "ir2 = model.encoder[3]\n",
    "convseq = ir2.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 24→144) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (144,24,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=144) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (144,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 144→24) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (24,144,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 3 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=144)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (24,56,56)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder3_ir2_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder3_ir2_output.txt\"), \"w\") as f:\n",
    "    for c in range(24):\n",
    "        for i in range(56):\n",
    "            for j in range(56):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc3_ir2 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fd5e0e-8c7c-49c1-83e9-e222ebaeba73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual3): HW output of Conv Stage 4\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e3d82bb-bd19-4c7e-aa55-d53b78b17493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 4 for enc4_ir3)\n",
    "ir3 = model.encoder[4]        # InvertedResidual (your \"enc4_ir3\")\n",
    "convseq = ir3.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 24→144) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (144,24,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,24,144)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=144) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (144,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=144, so IN=1 per channel\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,144)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 144→32) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (32,144,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,144,32)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57995865-16b3-441f-ad6f-269963192d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc4_ir3_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc4_ir3_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc4_ir3_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc4_ir3_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc4_ir3_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc4_ir3_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 24→144)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc4_ir3_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc4_ir3_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc4_ir3_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc4_ir3_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=2, groups=144)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc4_ir3_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc4_ir3_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc4_ir3_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc4_ir3_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 144→32)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc4_ir3_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc4_ir3_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc4_ir3_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc4_ir3_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b2dbee-e3c9-4a87-ae65-375f2c845893",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual3): SW output of Conv Stage 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23625115-73f3-4b34-b724-64e9b7cdba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\3134276204.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc4_ir3 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE3_OUT_PATH = \"software_output/sw_encoder3_ir2_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE3_OUT_PATH).astype(np.float32)   # shape (24,56,56)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,24,56,56)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[4] (InvertedResidual)\n",
    "ir3 = model.encoder[4]\n",
    "convseq = ir3.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 24→144) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (144,24,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=144) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (144,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 144→32) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (32,144,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 4 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=2)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=2, padding=1, groups=144)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (32,28,28)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder4_ir3_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder4_ir3_output.txt\"), \"w\") as f:\n",
    "    for c in range(32):\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc4_ir3 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07aa993-2b48-4cd5-b3b4-4676738ed282",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual4): HW output of Conv Stage 5\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14679393-fc88-4a77-8403-f7fa794483b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 5 for enc5_ir4)\n",
    "ir4 = model.encoder[5]        # InvertedResidual (your \"enc5_ir4\")\n",
    "convseq = ir4.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 32→192) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (192,32,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,32,192)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=192) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (192,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=192, so IN=1 per channel\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,192)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 192→32) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (32,192,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,192,32)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e8e371b-776b-4a34-bc05-9f76f1722209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc5_ir4_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc5_ir4_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc5_ir4_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc5_ir4_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc5_ir4_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc5_ir4_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 32→192)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc5_ir4_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc5_ir4_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc5_ir4_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc5_ir4_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=192)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc5_ir4_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc5_ir4_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc5_ir4_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc5_ir4_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 192→32)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc5_ir4_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc5_ir4_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc5_ir4_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc5_ir4_pw_b.h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2d3ab-2a61-48c0-8c3e-6c90269f493d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual4): SW output of Conv Stage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01a4fbe4-7939-4d18-a09a-9c49ee69325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\671504032.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc5_ir4 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE4_OUT_PATH = \"software_output/sw_encoder4_ir3_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE4_OUT_PATH).astype(np.float32)   # shape (32,28,28)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,32,28,28)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[5] (InvertedResidual)\n",
    "ir4 = model.encoder[5]\n",
    "convseq = ir4.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 32→192) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (192,32,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=192) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (192,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 192→32) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (32,192,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 5 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=192)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (32,28,28)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder5_ir4_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder5_ir4_output.txt\"), \"w\") as f:\n",
    "    for c in range(32):\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc5_ir4 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09a516-036f-4d4b-bbaa-1cd4b9de3781",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual5: HW output of Conv Stage 6\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c520cc35-57a5-499b-abfe-42affe849e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 6 for enc6_ir5)\n",
    "ir5 = model.encoder[6]        # InvertedResidual (your \"enc6_ir5\")\n",
    "convseq = ir5.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 32→192) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (192,32,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,32,192)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=192) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (192,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=192, so IN=1 per channel\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,192)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 192→32) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (32,192,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,192,32)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba60fb6e-49d6-40c4-919a-25ef9d5bc815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc6_ir5_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc6_ir5_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc6_ir5_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc6_ir5_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc6_ir5_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc6_ir5_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 32→192)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc6_ir5_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc6_ir5_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc6_ir5_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc6_ir5_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=192)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc6_ir5_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc6_ir5_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc6_ir5_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc6_ir5_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 192→32)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc6_ir5_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc6_ir5_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc6_ir5_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc6_ir5_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41133169-b6da-4144-a207-5e7783b879e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual5): SW output of Conv Stage 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aaaf956-f1be-41f5-b8f5-207e336bbe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\2364435396.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc6_ir5 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE5_OUT_PATH = \"software_output/sw_encoder5_ir4_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE5_OUT_PATH).astype(np.float32)   # shape (32,28,28)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,32,28,28)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[6] (InvertedResidual)\n",
    "ir5 = model.encoder[6]\n",
    "convseq = ir5.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 32→192) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (192,32,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=192) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (192,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 192→32) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (32,192,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 6 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=192)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (32,28,28)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder6_ir5_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder6_ir5_output.txt\"), \"w\") as f:\n",
    "    for c in range(32):\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc6_ir5 saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761dfac3-3ece-4435-96c8-c069680e238e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual6): HW output of Conv Stage 7\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bbfe5de-84da-4ec5-b484-2cd7572275e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 7 for enc7_ir6)\n",
    "ir6 = model.encoder[7]        # InvertedResidual (your \"enc7_ir6\")\n",
    "convseq = ir6.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 32→192) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (192,32,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,32,192)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=192) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (192,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=192, so IN=1 per channel\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,192)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 192→64) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (64,192,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,192,64)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9098adb-6d10-4d33-9a5a-efe26b19c668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc7_ir6_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc7_ir6_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc7_ir6_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc7_ir6_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc7_ir6_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc7_ir6_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 32→192)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc7_ir6_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc7_ir6_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc7_ir6_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc7_ir6_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=2, groups=192)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc7_ir6_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc7_ir6_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc7_ir6_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc7_ir6_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 192→64)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc7_ir6_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc7_ir6_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc7_ir6_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc7_ir6_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c978aa18-b816-46fc-b780-f89ea8cc729c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual6): SW output of Conv Stage 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24987760-1a19-493f-851d-5a90cce6e1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\469053632.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc7_ir6 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE6_OUT_PATH = \"software_output/sw_encoder6_ir5_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE6_OUT_PATH).astype(np.float32)   # shape (32,28,28)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,32,28,28)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[7] (InvertedResidual)\n",
    "ir6 = model.encoder[7]\n",
    "convseq = ir6.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 32→192) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (192,32,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=192) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (192,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 192→64) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (64,192,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 7 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=2)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=2, padding=1, groups=192)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (64,14,14)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder7_ir6_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder7_ir6_output.txt\"), \"w\") as f:\n",
    "    for c in range(64):\n",
    "        for i in range(14):\n",
    "            for j in range(14):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc7_ir6 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34baa0-5237-48e0-8e14-5f2144b978cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual7): HW output of Conv Stage 8\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba4f2825-fb55-4e2d-ad96-1e4125886ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 8 for enc8_ir7)\n",
    "ir7 = model.encoder[8]        # InvertedResidual (your \"enc8_ir7\")\n",
    "convseq = ir7.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,64,384)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=384\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,384)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→64) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (64,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,384,64)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a682c68-dcf1-449d-bcf8-5411b81d500d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc8_ir7_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc8_ir7_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc8_ir7_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc8_ir7_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc8_ir7_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc8_ir7_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 64→384)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc8_ir7_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc8_ir7_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc8_ir7_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc8_ir7_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=384)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc8_ir7_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc8_ir7_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc8_ir7_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc8_ir7_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 384→64)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc8_ir7_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc8_ir7_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc8_ir7_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc8_ir7_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edcf98-8b93-4301-a552-40330df9e8a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual7): SW output of Conv Stage 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9368732-8df8-41b9-94ac-b20bf754e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\2873620135.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc8_ir7 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE7_OUT_PATH = \"software_output/sw_encoder7_ir6_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE7_OUT_PATH).astype(np.float32)   # shape (64,14,14)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,64,14,14)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[8] (InvertedResidual)\n",
    "ir7 = model.encoder[8]\n",
    "convseq = ir7.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→64) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (64,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 8 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=384)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (64,14,14)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder8_ir7_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder8_ir7_output.txt\"), \"w\") as f:\n",
    "    for c in range(64):\n",
    "        for i in range(14):\n",
    "            for j in range(14):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc8_ir7 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec2f53-a0ca-4afe-bf52-7d9dbb981763",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual8): HW output of Conv Stage 9\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85d67501-efdc-472b-ac7a-9cb6f5e7af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 9 for enc9_ir8)\n",
    "ir8 = model.encoder[9]        # InvertedResidual (your \"enc9_ir8\")\n",
    "convseq = ir8.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,64,384)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=384\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,384)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→64) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (64,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,384,64)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2217a80c-317e-4cd7-9645-0c336f28d842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc9_ir8_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc9_ir8_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc9_ir8_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc9_ir8_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc9_ir8_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc9_ir8_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 64→384)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc9_ir8_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc9_ir8_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc9_ir8_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc9_ir8_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=384)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc9_ir8_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc9_ir8_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc9_ir8_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc9_ir8_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 384→64)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc9_ir8_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc9_ir8_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc9_ir8_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc9_ir8_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d501b-4896-44c6-b2c7-89be876eee22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual8): SW output of Conv Stage 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee4f240c-4e98-4b00-afeb-1d4461d7d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\3943399757.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc9_ir8 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE8_OUT_PATH = \"software_output/sw_encoder8_ir7_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE8_OUT_PATH).astype(np.float32)   # shape (64,14,14)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,64,14,14)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[9] (InvertedResidual)\n",
    "ir8 = model.encoder[9]\n",
    "convseq = ir8.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→64) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (64,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 9 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=384)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (64,14,14)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder9_ir8_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder9_ir8_output.txt\"), \"w\") as f:\n",
    "    for c in range(64):\n",
    "        for i in range(14):\n",
    "            for j in range(14):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc9_ir8 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17725f-4cec-4213-a067-e55b3a5accd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual9): HW output of Conv Stage 10\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77b0d59c-e659-4e45-aeb5-f94fd37e8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 10 for enc10_ir9)\n",
    "ir9 = model.encoder[10]       # InvertedResidual (your \"enc10_ir9\")\n",
    "convseq = ir9.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,64,384)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=384\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,384)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→64) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (64,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,384,64)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c68e7f0-a84b-4a9f-ae91-e5c3f29d33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc10_ir9_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc10_ir9_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc10_ir9_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc10_ir9_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc10_ir9_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc10_ir9_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 64→384)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc10_ir9_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc10_ir9_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc10_ir9_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc10_ir9_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=384)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc10_ir9_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc10_ir9_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc10_ir9_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc10_ir9_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 384→64)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc10_ir9_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc10_ir9_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc10_ir9_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc10_ir9_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b858332a-2c1c-4d44-82ef-97f26fd60bf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual9): SW output of Conv Stage 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e05babe-5ee8-412b-bece-9268bed010b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\3882749862.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc10_ir9 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE9_OUT_PATH = \"software_output/sw_encoder9_ir8_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE9_OUT_PATH).astype(np.float32)   # shape (64,14,14)\n",
    "x = np.expand_dims(x, axis=0)                     # (1,64,14,14)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[10] (InvertedResidual)\n",
    "ir9 = model.encoder[10]\n",
    "convseq = ir9.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→64) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (64,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 10 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=384)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (64,14,14)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder10_ir9_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder10_ir9_output.txt\"), \"w\") as f:\n",
    "    for c in range(64):\n",
    "        for i in range(14):\n",
    "            for j in range(14):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc10_ir9 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d941a1-eed3-40b9-9108-8e82e3f73c3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual10): HW output of Conv Stage 11\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73997eaf-26a7-48cc-acc0-89840dbce5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 11 for enc11_ir10)\n",
    "ir10 = model.encoder[11]       # InvertedResidual (your \"enc11_ir10\")\n",
    "convseq = ir10.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,64,384)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=384\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,384)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→96) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (96,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,384,96)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94007e48-baff-41cb-a4d7-f70b9b3ccc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc11_ir10_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc11_ir10_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc11_ir10_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc11_ir10_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc11_ir10_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc11_ir10_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 64→384)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc11_ir10_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc11_ir10_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc11_ir10_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc11_ir10_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=384)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc11_ir10_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc11_ir10_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc11_ir10_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc11_ir10_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 384→96)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc11_ir10_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc11_ir10_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc11_ir10_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc11_ir10_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71a7d5-1d33-41ca-abce-84e7134bf04b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidua10): SW output of Conv Stage 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9777b68d-1a0e-47db-afcb-f01481c4f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\1750625986.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc11_ir10 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE10_OUT_PATH = \"software_output/sw_encoder10_ir9_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE10_OUT_PATH).astype(np.float32)   # shape (64,14,14)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,64,14,14)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[11] (InvertedResidual)\n",
    "ir10 = model.encoder[11]\n",
    "convseq = ir10.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 64→384) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,64,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=384) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (384,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 384→96) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (96,384,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 11 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=384)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (96,14,14)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder11_ir10_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder11_ir10_output.txt\"), \"w\") as f:\n",
    "    for c in range(96):\n",
    "        for i in range(14):\n",
    "            for j in range(14):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc11_ir10 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe49e0d-1fc0-4538-ab41-8873b9d896a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual11): HW output of Conv Stage 12\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d73ed5a-b32d-4ba9-9d57-e293dca277c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 12 for enc12_ir11)\n",
    "ir11 = model.encoder[12]       # InvertedResidual (your \"enc12_ir11\")\n",
    "convseq = ir11.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 96→576) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,96,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,96,576)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=576) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=576\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,576)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 576→96) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (96,576,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,576,96)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fcfda18-aef0-48b2-bb4b-e946eca02e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc12_ir11_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc12_ir11_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc12_ir11_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc12_ir11_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc12_ir11_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc12_ir11_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 96→576)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc12_ir11_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc12_ir11_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc12_ir11_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc12_ir11_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=576)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc12_ir11_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc12_ir11_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc12_ir11_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc12_ir11_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 576→96)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc12_ir11_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc12_ir11_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc12_ir11_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc12_ir11_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddfac8-709b-47c9-bc13-dde0d566f50e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual 11): SW output of Conv Stage 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50910e98-8ec4-4a88-8d1d-47aff1980704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\1908892560.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc12_ir11 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE11_OUT_PATH = \"software_output/sw_encoder11_ir10_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE11_OUT_PATH).astype(np.float32)   # shape (96,14,14)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,96,14,14)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[12] (InvertedResidual)\n",
    "ir11 = model.encoder[12]\n",
    "convseq = ir11.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 96→576) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (576,96,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=576) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (576,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 576→96) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (96,576,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 12 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=576)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (96,14,14)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder12_ir11_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder12_ir11_output.txt\"), \"w\") as f:\n",
    "    for c in range(96):\n",
    "        for i in range(14):\n",
    "            for j in range(14):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc12_ir11 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503b856-d80b-4180-a673-a4409fc5ddbc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual12): HW output of Conv Stage 13\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "753bc6b4-cf2b-487e-8731-770193803ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 13 for enc13_ir12)\n",
    "ir12 = model.encoder[13]       # InvertedResidual (your \"enc13_ir12\")\n",
    "convseq = ir12.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 96→576) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,96,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,96,576)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=576) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise has groups=576\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))  # (3,3,1,576)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 576→96) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (96,576,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,576,96)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b037e1e6-8aaa-404c-b13a-b86db627d8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc13_ir12_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc13_ir12_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc13_ir12_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc13_ir12_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc13_ir12_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc13_ir12_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 96→576)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc13_ir12_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc13_ir12_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc13_ir12_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc13_ir12_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=576)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc13_ir12_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc13_ir12_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc13_ir12_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc13_ir12_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 576→96)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc13_ir12_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc13_ir12_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc13_ir12_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc13_ir12_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a77b35-a9f1-479c-9bd7-0d10d276ef53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual12): SW output of Conv Stage 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8fa04086-a98b-4885-ab05-a27acf336358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\1158603324.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc13_ir12 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE12_OUT_PATH = \"software_output/sw_encoder12_ir11_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE12_OUT_PATH).astype(np.float32)   # shape (96,14,14)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,96,14,14)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[13] (InvertedResidual)\n",
    "ir12 = model.encoder[13]\n",
    "convseq = ir12.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 96→576) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (576,96,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=576) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (576,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 576→96) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (96,576,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 13 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=576)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (96,14,14)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder13_ir12_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder13_ir12_output.txt\"), \"w\") as f:\n",
    "    for c in range(96):\n",
    "        for i in range(14):\n",
    "            for j in range(14):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc13_ir12 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e0c9b-6eb0-487b-9bef-32462d3d471c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual13): HW output of Conv Stage 14\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a52188f-facd-4904-b137-bf7e786be459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 14 for enc14_ir13)\n",
    "ir13 = model.encoder[14]       # InvertedResidual (your \"enc14_ir13\")\n",
    "convseq = ir13.conv            # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 96→576) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,96,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,96,576)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=576) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (3,3,1,576)\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 576→160) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (160,576,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,576,160)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be6cc659-45a2-4440-afca-a4eebbb01487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc14_ir13_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc14_ir13_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc14_ir13_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc14_ir13_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc14_ir13_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc14_ir13_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 96→576)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc14_ir13_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc14_ir13_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc14_ir13_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc14_ir13_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=2, groups=576)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc14_ir13_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc14_ir13_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc14_ir13_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc14_ir13_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 576→160)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc14_ir13_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc14_ir13_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc14_ir13_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc14_ir13_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892af11-0d59-41e8-8790-1c214e12521c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual13): SW output of Conv Stage 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0250b9d5-5f39-4918-884b-47d9d1e2b4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\2480937698.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc14_ir13 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE13_OUT_PATH = \"software_output/sw_encoder13_ir12_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE13_OUT_PATH).astype(np.float32)   # shape (96,14,14)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,96,14,14)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[14] (InvertedResidual)\n",
    "ir13 = model.encoder[14]\n",
    "convseq = ir13.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 96→576) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)  # (576,96,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=2, groups=576) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (576,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 576→160) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)  # (160,576,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 14 (expand → depthwise (stride=2) → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=2, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=2, padding=1, groups=576)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation here\n",
    "\n",
    "# Save results\n",
    "y_np = y.squeeze(0).cpu().numpy()  # (160,7,7)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder14_ir13_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder14_ir13_output.txt\"), \"w\") as f:\n",
    "    for c in range(160):\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc14_ir13 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57feeb1-43a8-44fa-bbe6-9799ea22a4fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual14): HW output of Conv Stage 15\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e1fe67e-7c1e-4851-b8d8-2a11217d688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 14 for enc15_ir14)\n",
    "ir14 = model.encoder[15]        # InvertedResidual (your \"enc15_ir14\")\n",
    "convseq = ir14.conv             # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 96→576) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,96,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,96,576)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=576) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (576,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise groups=576 → (3,3,1,576)\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 576→160) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (160,576,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,576,160)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c716e5e3-a850-4ee6-aaf9-f4007fa7f43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc15_ir14_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc15_ir14_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc15_ir14_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc15_ir14_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc15_ir14_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc15_ir14_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 96→576)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc15_ir14_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc15_ir14_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc15_ir14_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc15_ir14_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=576)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc15_ir14_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc15_ir14_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc15_ir14_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc15_ir14_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 576→160)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc15_ir14_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc15_ir14_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc15_ir14_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc15_ir14_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16379d-b4f7-4625-a446-47e52e844fcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual14): SW output of Conv Stage 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99635d47-775b-48c7-a102-7407d7b4063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\203865989.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc15_ir14 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE14_OUT_PATH = \"software_output/sw_encoder14_ir13_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE14_OUT_PATH).astype(np.float32)   # shape (160,7,7)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,160,7,7)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[15] (InvertedResidual)\n",
    "ir14 = model.encoder[15]\n",
    "convseq = ir14.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 160→960) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=960) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 960→160) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 15 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=960)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation\n",
    "\n",
    "# -------------------------------\n",
    "# Save output\n",
    "# -------------------------------\n",
    "y_np = y.squeeze(0).cpu().numpy()  # shape (160,7,7)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder15_ir14_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder15_ir14_output.txt\"), \"w\") as f:\n",
    "    for c in range(160):\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc15_ir14 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a0faf-b0e3-4365-871a-d7413b844311",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual15): HW output of Conv Stage 16\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf2b292a-f898-49bf-8428-b4208b65d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 16 for enc16_ir15)\n",
    "ir15 = model.encoder[16]        # InvertedResidual (your \"enc16_ir15\")\n",
    "convseq = ir15.conv             # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 160→960) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (960,160,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,160,960)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=960) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (960,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise groups=960 → (3,3,1,960)\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 960→160) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (160,960,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,960,160)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dbfa365e-c9fc-4e3b-aeb0-f7f27cecb989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc16_ir15_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc16_ir15_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc16_ir15_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc16_ir15_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc16_ir15_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc16_ir15_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 160→960)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc16_ir15_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc16_ir15_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc16_ir15_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc16_ir15_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=960)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc16_ir15_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc16_ir15_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc16_ir15_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc16_ir15_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 960→160)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc16_ir15_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc16_ir15_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc16_ir15_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc16_ir15_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ed003-f0f9-4201-b7c2-a07520cb6faa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual15): SW output of Conv Stage 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45616e7c-f8b3-47b2-9f78-fc4593d2255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\4261121454.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc16_ir15 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE15_OUT_PATH = \"software_output/sw_encoder15_ir14_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE15_OUT_PATH).astype(np.float32)   # shape (160,7,7)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,160,7,7)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[16] (InvertedResidual)\n",
    "ir15 = model.encoder[16]\n",
    "convseq = ir15.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 160→960) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=960) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 960→160) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 16 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=960)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation\n",
    "\n",
    "# -------------------------------\n",
    "# Save output\n",
    "# -------------------------------\n",
    "y_np = y.squeeze(0).cpu().numpy()  # shape (160,7,7)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder16_ir15_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder16_ir15_output.txt\"), \"w\") as f:\n",
    "    for c in range(160):\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc16_ir15 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc2a06-8fbf-4ecb-b2e9-d5de0dea75df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual16): HW output of Conv Stage 17\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a13ac63-2fe0-4e68-9d1d-9f7f10be4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (InvertedResidual at index 17 for enc17_ir16)\n",
    "ir16 = model.encoder[17]        # InvertedResidual (your \"enc17_ir16\")\n",
    "convseq = ir16.conv             # Sequential inside it\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 160→960) ----\n",
    "exp_conv: nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (960,160,1,1)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = (gamma_exp / np.sqrt(var_exp + eps_exp)).astype(\"float32\")\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(\"float32\")\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,160,960)\n",
    "W_exp_hls = np.transpose(W_exp_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=960) ----\n",
    "dw_conv: nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (960,1,3,3)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = (gamma_dw / np.sqrt(var_dw + eps_dw)).astype(\"float32\")\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(\"float32\")\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) — depthwise groups=960 → (3,3,1,960)\n",
    "W_dw_hls = np.transpose(W_dw_fold, (2, 3, 1, 0))\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 960→320) ----\n",
    "pw_conv: nn.Conv2d = convseq[2]\n",
    "pw_bn:   nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(\"float32\")  # (320,960,1,1)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = (gamma_pw / np.sqrt(var_pw + eps_pw)).astype(\"float32\")\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(\"float32\")\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,960,320)\n",
    "W_pw_hls = np.transpose(W_pw_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "666f5e94-c462-4a0c-ab6f-226894f7920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc17_ir16_exp_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc17_ir16_exp_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc17_ir16_dw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc17_ir16_dw_b.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc17_ir16_pw_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc17_ir16_pw_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expansion (1x1, 160→960)\n",
    "write_header_plain_numbers(W_exp_hls, \"enc17_ir16_exp_w\", CTYPE, os.path.join(OUT_DIR, \"enc17_ir16_exp_w.h\"))\n",
    "write_header_plain_numbers(b_exp_fold, \"enc17_ir16_exp_b\", CTYPE, os.path.join(OUT_DIR, \"enc17_ir16_exp_b.h\"))\n",
    "\n",
    "# Depthwise (3x3, stride=1, groups=960)\n",
    "write_header_plain_numbers(W_dw_hls, \"enc17_ir16_dw_w\", CTYPE, os.path.join(OUT_DIR, \"enc17_ir16_dw_w.h\"))\n",
    "write_header_plain_numbers(b_dw_fold, \"enc17_ir16_dw_b\", CTYPE, os.path.join(OUT_DIR, \"enc17_ir16_dw_b.h\"))\n",
    "\n",
    "# Projection (1x1, 960→320)\n",
    "write_header_plain_numbers(W_pw_hls, \"enc17_ir16_pw_w\", CTYPE, os.path.join(OUT_DIR, \"enc17_ir16_pw_w.h\"))\n",
    "write_header_plain_numbers(b_pw_fold, \"enc17_ir16_pw_b\", CTYPE, os.path.join(OUT_DIR, \"enc17_ir16_pw_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf42c4-b224-406b-ab24-b0bc0a1e0644",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual16): SW output of Conv Stage 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27d509c9-5c83-4743-8f69-ce29acf351ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\2769533877.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc17_ir16 saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE16_OUT_PATH = \"software_output/sw_encoder16_ir15_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE16_OUT_PATH).astype(np.float32)   # shape (160,7,7)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,160,7,7)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[17] (InvertedResidual)\n",
    "ir16 = model.encoder[17]\n",
    "convseq = ir16.conv\n",
    "\n",
    "# ---- Expansion Conv + BN (1x1, 160→960) ----\n",
    "exp_conv: torch.nn.Conv2d = convseq[0][0]\n",
    "exp_bn:   torch.nn.BatchNorm2d = convseq[0][1]\n",
    "\n",
    "W_exp = exp_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_exp = exp_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_exp  = exp_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_exp  = exp_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_exp   = exp_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_exp   = float(exp_bn.eps)\n",
    "\n",
    "s_exp = gamma_exp / np.sqrt(var_exp + eps_exp)\n",
    "W_exp_fold = (W_exp * s_exp[:, None, None, None]).astype(np.float16)\n",
    "b_exp_fold = (beta_exp - mean_exp * s_exp).astype(np.float16)\n",
    "\n",
    "# ---- Depthwise Conv + BN (3x3, stride=1, groups=960) ----\n",
    "dw_conv: torch.nn.Conv2d = convseq[1][0]\n",
    "dw_bn:   torch.nn.BatchNorm2d = convseq[1][1]\n",
    "\n",
    "W_dw = dw_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_dw = dw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_dw  = dw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_dw  = dw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_dw   = dw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_dw   = float(dw_bn.eps)\n",
    "\n",
    "s_dw = gamma_dw / np.sqrt(var_dw + eps_dw)\n",
    "W_dw_fold = (W_dw * s_dw[:, None, None, None]).astype(np.float16)\n",
    "b_dw_fold = (beta_dw - mean_dw * s_dw).astype(np.float16)\n",
    "\n",
    "# ---- Projection Conv + BN (1x1, 960→320) ----\n",
    "pw_conv: torch.nn.Conv2d = convseq[2]\n",
    "pw_bn:   torch.nn.BatchNorm2d = convseq[3]\n",
    "\n",
    "W_pw = pw_conv.weight.detach().cpu().numpy().astype(np.float32)\n",
    "gamma_pw = pw_bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta_pw  = pw_bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean_pw  = pw_bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var_pw   = pw_bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps_pw   = float(pw_bn.eps)\n",
    "\n",
    "s_pw = gamma_pw / np.sqrt(var_pw + eps_pw)\n",
    "W_pw_fold = (W_pw * s_pw[:, None, None, None]).astype(np.float16)\n",
    "b_pw_fold = (beta_pw - mean_pw * s_pw).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_exp_torch = torch.tensor(W_exp_fold, dtype=torch.float16, device=device)\n",
    "b_exp_torch = torch.tensor(b_exp_fold, dtype=torch.float16, device=device)\n",
    "W_dw_torch  = torch.tensor(W_dw_fold,  dtype=torch.float16, device=device)\n",
    "b_dw_torch  = torch.tensor(b_dw_fold,  dtype=torch.float16, device=device)\n",
    "W_pw_torch  = torch.tensor(W_pw_fold,  dtype=torch.float16, device=device)\n",
    "b_pw_torch  = torch.tensor(b_pw_fold,  dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 17 (expand → depthwise → projection)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    # Expansion\n",
    "    y = F.conv2d(x_torch, W_exp_torch, bias=b_exp_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Depthwise (stride=1, padding=1)\n",
    "    y = F.conv2d(y, W_dw_torch, bias=b_dw_torch, stride=1, padding=1, groups=960)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "    # Projection\n",
    "    y = F.conv2d(y, W_pw_torch, bias=b_pw_torch, stride=1, padding=0)\n",
    "    # no activation\n",
    "\n",
    "# -------------------------------\n",
    "# Save output\n",
    "# -------------------------------\n",
    "y_np = y.squeeze(0).cpu().numpy()  # shape (320,7,7)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder17_ir16_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder17_ir16_output.txt\"), \"w\") as f:\n",
    "    for c in range(320):\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc17_ir16 saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b591d39-0647-4604-93e9-5f9c3de6b531",
   "metadata": {},
   "source": [
    "### (invertedresidual17): HW output of Conv Stage 18\n",
    "     1. Save Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b4121eb-a2f9-4cd2-b31a-d4e1ea585121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get encoder stage (Conv2dNormActivation at index 18 for enc18_cnv)\n",
    "cnv18 = model.encoder[18]       # Conv2dNormActivation\n",
    "conv: nn.Conv2d = cnv18[0]      # Conv2d(320→1280, 1x1)\n",
    "bn:   nn.BatchNorm2d = cnv18[1] # BatchNorm2d(1280)\n",
    "\n",
    "# ---- Conv + BN (1x1, 320→1280) ----\n",
    "W = conv.weight.detach().cpu().numpy().astype(\"float32\")   # (1280,320,1,1)\n",
    "gamma = bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta  = bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean  = bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var   = bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps   = float(bn.eps)\n",
    "\n",
    "s = (gamma / np.sqrt(var + eps)).astype(\"float32\")\n",
    "W_fold = (W * s[:, None, None, None]).astype(\"float32\")\n",
    "b_fold = (beta - mean * s).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C) → (1,1,320,1280)\n",
    "W_hls = np.transpose(W_fold, (2, 3, 1, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b003356-f3c8-4923-80ce-a4b58c64af78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc18_cnv_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\enc18_cnv_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Conv (1x1, 320→1280)\n",
    "write_header_plain_numbers(W_hls, \"enc18_cnv_w\", CTYPE, os.path.join(OUT_DIR, \"enc18_cnv_w.h\"))\n",
    "write_header_plain_numbers(b_fold, \"enc18_cnv_b\", CTYPE, os.path.join(OUT_DIR, \"enc18_cnv_b.h\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba14e4-8620-4786-92f8-3fe4359c7b41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### (invertedresidual17): SW output of Conv Stage 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0340a8fc-6313-42f3-86ea-262e2cfe04ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_25464\\35019247.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Software output for enc18_cnv saved.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "STAGE17_OUT_PATH = \"software_output/sw_encoder17_ir16_output.npy\"\n",
    "WEIGHT_SAVE_DIR = \"software_output\"\n",
    "os.makedirs(WEIGHT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD PREVIOUS STAGE OUTPUT\n",
    "# -------------------------------\n",
    "x = np.load(STAGE17_OUT_PATH).astype(np.float32)   # shape (320,7,7)\n",
    "x = np.expand_dims(x, axis=0)                      # (1,320,7,7)\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# -------------------------------\n",
    "# Load model + weights\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Extract encoder[18] (Conv2dNormActivation)\n",
    "cnv18 = model.encoder[18]\n",
    "conv: torch.nn.Conv2d = cnv18[0]   # Conv2d(320→1280, 1x1)\n",
    "bn:   torch.nn.BatchNorm2d = cnv18[1]\n",
    "\n",
    "# ---- Conv + BN (1x1, 320→1280) ----\n",
    "W = conv.weight.detach().cpu().numpy().astype(np.float32)  # (1280,320,1,1)\n",
    "gamma = bn.weight.detach().cpu().numpy().astype(np.float32)\n",
    "beta  = bn.bias.detach().cpu().numpy().astype(np.float32)\n",
    "mean  = bn.running_mean.detach().cpu().numpy().astype(np.float32)\n",
    "var   = bn.running_var.detach().cpu().numpy().astype(np.float32)\n",
    "eps   = float(bn.eps)\n",
    "\n",
    "s = gamma / np.sqrt(var + eps)\n",
    "W_fold = (W * s[:, None, None, None]).astype(np.float16)\n",
    "b_fold = (beta - mean * s).astype(np.float16)\n",
    "\n",
    "# Convert to torch tensors\n",
    "W_torch = torch.tensor(W_fold, dtype=torch.float16, device=device)\n",
    "b_torch = torch.tensor(b_fold, dtype=torch.float16, device=device)\n",
    "\n",
    "x_torch = x_torch.to(device).to(torch.float16)\n",
    "\n",
    "# -------------------------------\n",
    "# Run stage 18 (1x1 conv + ReLU6)\n",
    "# -------------------------------\n",
    "with torch.no_grad():\n",
    "    y = F.conv2d(x_torch, W_torch, bias=b_torch, stride=1, padding=0)\n",
    "    y = torch.clamp(y, 0, 6)  # ReLU6\n",
    "\n",
    "# -------------------------------\n",
    "# Save output\n",
    "# -------------------------------\n",
    "y_np = y.squeeze(0).cpu().numpy()  # shape (1280,7,7)\n",
    "np.save(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder18_cnv_output.npy\"), y_np)\n",
    "\n",
    "with open(os.path.join(WEIGHT_SAVE_DIR, \"sw_encoder18_cnv_output.txt\"), \"w\") as f:\n",
    "    for c in range(1280):\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"✅ Software output for enc18_cnv saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bad3ea-eebe-489f-a89c-c6c1b70c60b1",
   "metadata": {},
   "source": [
    "### Decoder Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44f2d0-f794-4b6a-afba-926a9116d79e",
   "metadata": {},
   "source": [
    "### Dec 0 Hardware Weights / Biases Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f69970dd-c635-411b-a86f-0fd47c787607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get decoder stage 0 (ConvTranspose2d + BN + ReLU)\n",
    "dec0 = model.seg_head[0:2]      # ConvTranspose2d + BatchNorm2d\n",
    "deconv: nn.ConvTranspose2d = dec0[0]  # ConvTranspose2d(1280→256, k=2, s=2)\n",
    "bn:     nn.BatchNorm2d = dec0[1]      # BatchNorm2d(256)\n",
    "\n",
    "# ---- ConvTranspose2d + BN folding ----\n",
    "W = deconv.weight.detach().cpu().numpy().astype(\"float32\")  # (1280,256,2,2) in PyTorch layout\n",
    "b = deconv.bias\n",
    "if b is not None:\n",
    "    b = b.detach().cpu().numpy().astype(\"float32\")\n",
    "else:\n",
    "    b = np.zeros(W.shape[1], dtype=np.float32)  # if no bias in conv\n",
    "\n",
    "gamma = bn.weight.detach().cpu().numpy().astype(\"float32\")\n",
    "beta  = bn.bias.detach().cpu().numpy().astype(\"float32\")\n",
    "mean  = bn.running_mean.detach().cpu().numpy().astype(\"float32\")\n",
    "var   = bn.running_var.detach().cpu().numpy().astype(\"float32\")\n",
    "eps   = float(bn.eps)\n",
    "\n",
    "# Fold BN\n",
    "s = (gamma / np.sqrt(var + eps)).astype(\"float32\")\n",
    "W_fold = (W * s[None, :, None, None]).astype(\"float32\")   # broadcast scale over OUT_C\n",
    "b_fold = (beta + (b - mean) * s).astype(\"float32\")\n",
    "\n",
    "# Layout for HLS: (K,K,IN_C,OUT_C)\n",
    "# PyTorch ConvTranspose2d weights are (in_ch, out_ch, kH, kW)\n",
    "# We want (kH, kW, in_ch, out_ch) for HLS\n",
    "W_hls = np.transpose(W_fold, (2, 3, 0, 1))  # (2,2,1280,256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4d384e-14e6-44bd-9401-2aab867e23e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\dec0_w.h\n",
      "Wrote: C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\dec0_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"\n",
    "CTYPE   = \"data_t\"  # ap_fixed typedef in HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            return \"{\" + \",\".join(f\"{x:.6f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f\"{HEADER_INCLUDE}\\n\\n\")\n",
    "        f.write(\"// Auto-generated with BN folded\\n\\n\")\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    print(\"Wrote:\", os.path.abspath(out_path))\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ConvTranspose2d (2x2, 1280→256)\n",
    "write_header_plain_numbers(W_hls, \"dec0_w\", CTYPE, os.path.join(OUT_DIR, \"dec0_w.h\"))\n",
    "write_header_plain_numbers(b_fold, \"dec0_b\", CTYPE, os.path.join(OUT_DIR, \"dec0_b.h\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350288c-e183-40c5-9061-d37b14621512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197b3ba-ef22-404b-9800-dd21cb40932f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f1b3e5-1b24-4a1c-9526-2807f1d4bfab",
   "metadata": {},
   "source": [
    "### Dec 0 Software Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16cad8-44c9-4d21-bc09-285f9f7765f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6717de90-d53b-4235-9556-819338bdaff1",
   "metadata": {},
   "source": [
    "### UNUSED CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c576bd8-15ca-4576-a4ad-6aefb586f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e1610-04d0-45f1-bca4-aaacf7acb594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
