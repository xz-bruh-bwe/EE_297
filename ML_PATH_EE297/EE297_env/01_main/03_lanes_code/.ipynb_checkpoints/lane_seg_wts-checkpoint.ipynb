{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7450c338-5eaa-4ad8-99e8-3b642e487765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e119a4b-ec76-4a35-b92b-4b150c9a3360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a561caa1-c124-4143-82cc-e6078f52cb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Baron\\\\Desktop\\\\EE_297_Repo\\\\EE_297\\\\ML_PATH_EE297\\\\EE297_env\\\\01_main\\\\03_lanes_code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205fbbb-9b09-4cbc-acf7-46920552871b",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad177fbc-b76b-4732-815e-81729dd76644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import Necessary Liobraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "from tqdm.auto import tqdm as notebook_tqdm\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, AveragePooling2D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#import torchvision.transforms as transforms\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fa35d-ef94-44b1-8bab-e4bb0ffd7d99",
   "metadata": {},
   "source": [
    "### RESIZING IMAGE (SW + HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc51230-f090-4c18-b5d2-a56b0210457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Input and output paths\n",
    "input_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og.jpg\"\n",
    "output_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\"\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(input_path)\n",
    "\n",
    "# Resize to 224x224\n",
    "resized_image = image.resize((224, 224))\n",
    "\n",
    "# Save the resized image\n",
    "resized_image.save(output_path)\n",
    "\n",
    "print(f\"Image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2efae-d8a8-4f97-8778-76fea27c721a",
   "metadata": {},
   "source": [
    "### CONVERTS IMAGE TO .TXT FILE (HW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07dc063-c513-4f34-ab6d-92bf075dbec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Image converted to channel-major .txt at:\n",
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_rgb_224.txt\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==== USER INPUT ====\n",
    "input_path = r\"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\test_imgs\\img_og_224.jpg\"\n",
    "output_basename = \"img_rgb_224.txt\"  # Output .txt filename\n",
    "target_size = (224, 224)               # Resize target\n",
    "\n",
    "# ==== LOAD IMAGE ====\n",
    "img = cv2.imread(input_path)\n",
    "if img is None:\n",
    "    raise FileNotFoundError(f\"Could not load image at: {input_path}\")\n",
    "\n",
    "img = cv2.resize(img, target_size)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR → RGB\n",
    "img = img.astype(np.float32) / 255.0        # Normalize to [0,1]\n",
    "\n",
    "# ==== WRITE TO TXT ====\n",
    "output_dir = os.path.dirname(input_path)\n",
    "output_path = os.path.join(output_dir, output_basename)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    for c in range(3):  # R, G, B\n",
    "        for i in range(target_size[1]):\n",
    "            for j in range(target_size[0]):\n",
    "                f.write(f\"{img[i, j, c]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "print(f\"[✓] Image converted to channel-major .txt at:\\n{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09eebd-7a8b-4f26-9107-5328fd256dbf",
   "metadata": {},
   "source": [
    "### MODEL CREATION (SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8b318cd-955a-4e69-9431-4c4d5b4c1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class LaneSegOnlyMobileNetV2(nn.Module):\n",
    "    def __init__(self, pretrained=True, freeze_stem=False):\n",
    "        super().__init__()\n",
    "        # ---- Encoder: MobileNetV2 ----\n",
    "        weights = models.MobileNet_V2_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        m = models.mobilenet_v2(weights=weights)\n",
    "\n",
    "        # Feature extractor (ends at last conv-bn-relu block)\n",
    "        self.encoder = m.features              # output: (B, 1280, 7, 7) for 224x224\n",
    "        enc_out_ch = 1280\n",
    "\n",
    "        if freeze_stem:\n",
    "            # freeze early, cheap speed/regularization trick\n",
    "            for p in list(self.encoder.parameters())[:]:\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # ---- Lightweight decoder (no skip connections) ----\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(enc_out_ch, 256, kernel_size=2, stride=2),  # 7 -> 14\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),         # 14 -> 28\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),          # 28 -> 56\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),           # 56 -> 112\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(32, 1, kernel_size=1),  # logits (B,1,112,112)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = x.shape[-2:]\n",
    "        feats = self.encoder(x)                 # (B,1280,7,7) at 224x224\n",
    "        seg   = self.seg_head(feats)           # (B,1,112,112)\n",
    "        seg   = F.interpolate(seg, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "        return seg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e33fb-c65f-45c6-8f03-1028412109ec",
   "metadata": {},
   "source": [
    "### LOAD IN TRAINED WEIGHTS FILE (SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45bf193c-cd33-4cf9-b954-5aa2985c594b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_47472\\2694822331.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LaneSegOnlyMobileNetV2(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (seg_head): Sequential(\n",
       "    (0): ConvTranspose2d(1280, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LaneSegOnlyMobileNetV2()\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\"))\n",
    "model.to(device)\n",
    "model.eval()  # set to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e58870c-37f4-48d9-9d03-af2bfbe36c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.0.0.weight torch.Size([32, 3, 3, 3])\n",
      "encoder.0.1.weight torch.Size([32])\n",
      "encoder.0.1.bias torch.Size([32])\n",
      "encoder.1.conv.0.0.weight torch.Size([32, 1, 3, 3])\n",
      "encoder.1.conv.0.1.weight torch.Size([32])\n",
      "encoder.1.conv.0.1.bias torch.Size([32])\n",
      "encoder.1.conv.1.weight torch.Size([16, 32, 1, 1])\n",
      "encoder.1.conv.2.weight torch.Size([16])\n",
      "encoder.1.conv.2.bias torch.Size([16])\n",
      "encoder.2.conv.0.0.weight torch.Size([96, 16, 1, 1])\n",
      "encoder.2.conv.0.1.weight torch.Size([96])\n",
      "encoder.2.conv.0.1.bias torch.Size([96])\n",
      "encoder.2.conv.1.0.weight torch.Size([96, 1, 3, 3])\n",
      "encoder.2.conv.1.1.weight torch.Size([96])\n",
      "encoder.2.conv.1.1.bias torch.Size([96])\n",
      "encoder.2.conv.2.weight torch.Size([24, 96, 1, 1])\n",
      "encoder.2.conv.3.weight torch.Size([24])\n",
      "encoder.2.conv.3.bias torch.Size([24])\n",
      "encoder.3.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "encoder.3.conv.0.1.weight torch.Size([144])\n",
      "encoder.3.conv.0.1.bias torch.Size([144])\n",
      "encoder.3.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "encoder.3.conv.1.1.weight torch.Size([144])\n",
      "encoder.3.conv.1.1.bias torch.Size([144])\n",
      "encoder.3.conv.2.weight torch.Size([24, 144, 1, 1])\n",
      "encoder.3.conv.3.weight torch.Size([24])\n",
      "encoder.3.conv.3.bias torch.Size([24])\n",
      "encoder.4.conv.0.0.weight torch.Size([144, 24, 1, 1])\n",
      "encoder.4.conv.0.1.weight torch.Size([144])\n",
      "encoder.4.conv.0.1.bias torch.Size([144])\n",
      "encoder.4.conv.1.0.weight torch.Size([144, 1, 3, 3])\n",
      "encoder.4.conv.1.1.weight torch.Size([144])\n",
      "encoder.4.conv.1.1.bias torch.Size([144])\n",
      "encoder.4.conv.2.weight torch.Size([32, 144, 1, 1])\n",
      "encoder.4.conv.3.weight torch.Size([32])\n",
      "encoder.4.conv.3.bias torch.Size([32])\n",
      "encoder.5.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.5.conv.0.1.weight torch.Size([192])\n",
      "encoder.5.conv.0.1.bias torch.Size([192])\n",
      "encoder.5.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.5.conv.1.1.weight torch.Size([192])\n",
      "encoder.5.conv.1.1.bias torch.Size([192])\n",
      "encoder.5.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "encoder.5.conv.3.weight torch.Size([32])\n",
      "encoder.5.conv.3.bias torch.Size([32])\n",
      "encoder.6.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.6.conv.0.1.weight torch.Size([192])\n",
      "encoder.6.conv.0.1.bias torch.Size([192])\n",
      "encoder.6.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.6.conv.1.1.weight torch.Size([192])\n",
      "encoder.6.conv.1.1.bias torch.Size([192])\n",
      "encoder.6.conv.2.weight torch.Size([32, 192, 1, 1])\n",
      "encoder.6.conv.3.weight torch.Size([32])\n",
      "encoder.6.conv.3.bias torch.Size([32])\n",
      "encoder.7.conv.0.0.weight torch.Size([192, 32, 1, 1])\n",
      "encoder.7.conv.0.1.weight torch.Size([192])\n",
      "encoder.7.conv.0.1.bias torch.Size([192])\n",
      "encoder.7.conv.1.0.weight torch.Size([192, 1, 3, 3])\n",
      "encoder.7.conv.1.1.weight torch.Size([192])\n",
      "encoder.7.conv.1.1.bias torch.Size([192])\n",
      "encoder.7.conv.2.weight torch.Size([64, 192, 1, 1])\n",
      "encoder.7.conv.3.weight torch.Size([64])\n",
      "encoder.7.conv.3.bias torch.Size([64])\n",
      "encoder.8.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.8.conv.0.1.weight torch.Size([384])\n",
      "encoder.8.conv.0.1.bias torch.Size([384])\n",
      "encoder.8.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.8.conv.1.1.weight torch.Size([384])\n",
      "encoder.8.conv.1.1.bias torch.Size([384])\n",
      "encoder.8.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.8.conv.3.weight torch.Size([64])\n",
      "encoder.8.conv.3.bias torch.Size([64])\n",
      "encoder.9.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.9.conv.0.1.weight torch.Size([384])\n",
      "encoder.9.conv.0.1.bias torch.Size([384])\n",
      "encoder.9.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.9.conv.1.1.weight torch.Size([384])\n",
      "encoder.9.conv.1.1.bias torch.Size([384])\n",
      "encoder.9.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.9.conv.3.weight torch.Size([64])\n",
      "encoder.9.conv.3.bias torch.Size([64])\n",
      "encoder.10.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.10.conv.0.1.weight torch.Size([384])\n",
      "encoder.10.conv.0.1.bias torch.Size([384])\n",
      "encoder.10.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.10.conv.1.1.weight torch.Size([384])\n",
      "encoder.10.conv.1.1.bias torch.Size([384])\n",
      "encoder.10.conv.2.weight torch.Size([64, 384, 1, 1])\n",
      "encoder.10.conv.3.weight torch.Size([64])\n",
      "encoder.10.conv.3.bias torch.Size([64])\n",
      "encoder.11.conv.0.0.weight torch.Size([384, 64, 1, 1])\n",
      "encoder.11.conv.0.1.weight torch.Size([384])\n",
      "encoder.11.conv.0.1.bias torch.Size([384])\n",
      "encoder.11.conv.1.0.weight torch.Size([384, 1, 3, 3])\n",
      "encoder.11.conv.1.1.weight torch.Size([384])\n",
      "encoder.11.conv.1.1.bias torch.Size([384])\n",
      "encoder.11.conv.2.weight torch.Size([96, 384, 1, 1])\n",
      "encoder.11.conv.3.weight torch.Size([96])\n",
      "encoder.11.conv.3.bias torch.Size([96])\n",
      "encoder.12.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.12.conv.0.1.weight torch.Size([576])\n",
      "encoder.12.conv.0.1.bias torch.Size([576])\n",
      "encoder.12.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.12.conv.1.1.weight torch.Size([576])\n",
      "encoder.12.conv.1.1.bias torch.Size([576])\n",
      "encoder.12.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "encoder.12.conv.3.weight torch.Size([96])\n",
      "encoder.12.conv.3.bias torch.Size([96])\n",
      "encoder.13.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.13.conv.0.1.weight torch.Size([576])\n",
      "encoder.13.conv.0.1.bias torch.Size([576])\n",
      "encoder.13.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.13.conv.1.1.weight torch.Size([576])\n",
      "encoder.13.conv.1.1.bias torch.Size([576])\n",
      "encoder.13.conv.2.weight torch.Size([96, 576, 1, 1])\n",
      "encoder.13.conv.3.weight torch.Size([96])\n",
      "encoder.13.conv.3.bias torch.Size([96])\n",
      "encoder.14.conv.0.0.weight torch.Size([576, 96, 1, 1])\n",
      "encoder.14.conv.0.1.weight torch.Size([576])\n",
      "encoder.14.conv.0.1.bias torch.Size([576])\n",
      "encoder.14.conv.1.0.weight torch.Size([576, 1, 3, 3])\n",
      "encoder.14.conv.1.1.weight torch.Size([576])\n",
      "encoder.14.conv.1.1.bias torch.Size([576])\n",
      "encoder.14.conv.2.weight torch.Size([160, 576, 1, 1])\n",
      "encoder.14.conv.3.weight torch.Size([160])\n",
      "encoder.14.conv.3.bias torch.Size([160])\n",
      "encoder.15.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.15.conv.0.1.weight torch.Size([960])\n",
      "encoder.15.conv.0.1.bias torch.Size([960])\n",
      "encoder.15.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.15.conv.1.1.weight torch.Size([960])\n",
      "encoder.15.conv.1.1.bias torch.Size([960])\n",
      "encoder.15.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "encoder.15.conv.3.weight torch.Size([160])\n",
      "encoder.15.conv.3.bias torch.Size([160])\n",
      "encoder.16.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.16.conv.0.1.weight torch.Size([960])\n",
      "encoder.16.conv.0.1.bias torch.Size([960])\n",
      "encoder.16.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.16.conv.1.1.weight torch.Size([960])\n",
      "encoder.16.conv.1.1.bias torch.Size([960])\n",
      "encoder.16.conv.2.weight torch.Size([160, 960, 1, 1])\n",
      "encoder.16.conv.3.weight torch.Size([160])\n",
      "encoder.16.conv.3.bias torch.Size([160])\n",
      "encoder.17.conv.0.0.weight torch.Size([960, 160, 1, 1])\n",
      "encoder.17.conv.0.1.weight torch.Size([960])\n",
      "encoder.17.conv.0.1.bias torch.Size([960])\n",
      "encoder.17.conv.1.0.weight torch.Size([960, 1, 3, 3])\n",
      "encoder.17.conv.1.1.weight torch.Size([960])\n",
      "encoder.17.conv.1.1.bias torch.Size([960])\n",
      "encoder.17.conv.2.weight torch.Size([320, 960, 1, 1])\n",
      "encoder.17.conv.3.weight torch.Size([320])\n",
      "encoder.17.conv.3.bias torch.Size([320])\n",
      "encoder.18.0.weight torch.Size([1280, 320, 1, 1])\n",
      "encoder.18.1.weight torch.Size([1280])\n",
      "encoder.18.1.bias torch.Size([1280])\n",
      "seg_head.0.weight torch.Size([1280, 256, 2, 2])\n",
      "seg_head.0.bias torch.Size([256])\n",
      "seg_head.1.weight torch.Size([256])\n",
      "seg_head.1.bias torch.Size([256])\n",
      "seg_head.3.weight torch.Size([256, 256, 3, 3])\n",
      "seg_head.3.bias torch.Size([256])\n",
      "seg_head.4.weight torch.Size([256])\n",
      "seg_head.4.bias torch.Size([256])\n",
      "seg_head.6.weight torch.Size([256, 128, 2, 2])\n",
      "seg_head.6.bias torch.Size([128])\n",
      "seg_head.7.weight torch.Size([128])\n",
      "seg_head.7.bias torch.Size([128])\n",
      "seg_head.9.weight torch.Size([128, 128, 3, 3])\n",
      "seg_head.9.bias torch.Size([128])\n",
      "seg_head.10.weight torch.Size([128])\n",
      "seg_head.10.bias torch.Size([128])\n",
      "seg_head.12.weight torch.Size([128, 64, 2, 2])\n",
      "seg_head.12.bias torch.Size([64])\n",
      "seg_head.13.weight torch.Size([64])\n",
      "seg_head.13.bias torch.Size([64])\n",
      "seg_head.15.weight torch.Size([64, 64, 3, 3])\n",
      "seg_head.15.bias torch.Size([64])\n",
      "seg_head.16.weight torch.Size([64])\n",
      "seg_head.16.bias torch.Size([64])\n",
      "seg_head.18.weight torch.Size([64, 32, 2, 2])\n",
      "seg_head.18.bias torch.Size([32])\n",
      "seg_head.19.weight torch.Size([32])\n",
      "seg_head.19.bias torch.Size([32])\n",
      "seg_head.21.weight torch.Size([32, 32, 3, 3])\n",
      "seg_head.21.bias torch.Size([32])\n",
      "seg_head.22.weight torch.Size([32])\n",
      "seg_head.22.bias torch.Size([32])\n",
      "seg_head.24.weight torch.Size([1, 32, 1, 1])\n",
      "seg_head.24.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f54444-fd92-439f-adea-af7fa5b34cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12b21c8f-5207-43a6-9139-040d6ae84f44",
   "metadata": {},
   "source": [
    "### (encoder): Sequential( HW Conv -> FOLD (BatchNorm) -> Relu)\n",
    "1. Save Weights File (HW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66cf1fbd-dea3-46c7-9f07-da325ec4d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv shape (O,I,K,K): (32, 3, 3, 3)\n",
      "BN shapes: (32,) (32,)\n"
     ]
    }
   ],
   "source": [
    "# 1) Get the encoder stage shown in your screenshot\n",
    "\n",
    "#    It's a Conv2dNormActivation with (0)=Conv2d, (1)=BatchNorm2d, (2)=ReLU6\n",
    "# encoder[0] = Conv2dNormActivation: [0]=Conv2d(bias=False), [1]=BatchNorm2d, [2]=ReLU6\n",
    "enc0 = model.encoder[0]\n",
    "conv: nn.Conv2d = enc0[0]\n",
    "bn:   nn.BatchNorm2d = enc0[1]\n",
    "\n",
    "assert isinstance(conv, nn.Conv2d)\n",
    "assert isinstance(bn,   nn.BatchNorm2d)\n",
    "\n",
    "print(\"Conv shape (O,I,K,K):\", tuple(conv.weight.shape))\n",
    "print(\"BN shapes:\", tuple(bn.weight.shape), tuple(bn.bias.shape))\n",
    "\n",
    "# Setting up BatchNorm Math Fold:\n",
    "# Get tensors (PyTorch conv layout: [O,I,K,K])\n",
    "W = conv.weight.detach().cpu().numpy().astype(\"float32\")          # (O,I,K,K)\n",
    "gamma = bn.weight.detach().cpu().numpy().astype(\"float32\")        # (O,)\n",
    "beta  = bn.bias.detach().cpu().numpy().astype(\"float32\")          # (O,)\n",
    "mean  = bn.running_mean.detach().cpu().numpy().astype(\"float32\")  # (O,)\n",
    "var   = bn.running_var.detach().cpu().numpy().astype(\"float32\")   # (O,)\n",
    "eps   = float(bn.eps)\n",
    "\n",
    "# Per-output scale\n",
    "s = (gamma / np.sqrt(var + eps)).astype(\"float32\")  # (O,)\n",
    "\n",
    "# Fold into conv (bias=False case)\n",
    "W_fold = (W * s[:, None, None, None]).astype(\"float32\")  # broadcast scale over I,K,K\n",
    "b_fold = (beta - mean * s).astype(\"float32\")             # (O,)\n",
    "\n",
    "# If your HLS expects (K,K,IN_C,OUT_C) layout:\n",
    "W_hls = np.transpose(W_fold, (2, 3, 1, 0))  # (K,K,I,O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "121a2fbb-db90-48d8-a435-29d183295eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\encoder_conv0_w.h\n",
      "C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\ML_PATH_EE297\\EE297_env\\01_main\\03_lanes_code\\weights\\encoder_conv0_b.h\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = \"weights\"      # relative path in your project\n",
    "CTYPE   = \"data_t\"       # ap_fixed typedef in your HLS\n",
    "\n",
    "HEADER_INCLUDE = r'#include \"C:\\Users\\Baron\\Desktop\\EE_297_Repo\\EE_297\\hardware_imp\\vitis_hls\\lane_seg_hls\\lane_seg_top.h\"'\n",
    "\n",
    "def write_header_plain_numbers(array: np.ndarray, var_name: str, ctype: str, out_path: str):\n",
    "    \"\"\"\n",
    "    Writes a header with:\n",
    "      - #pragma once\n",
    "      - your include path\n",
    "      - declaration: data_t var[... ] = { ... };\n",
    "      - numbers printed as plain literals (no 'f', no '(data_t)' casts)\n",
    "    \"\"\"\n",
    "    array = np.asarray(array, dtype=np.float32)\n",
    "    dims  = \"\".join(f\"[{d}]\" for d in array.shape)\n",
    "\n",
    "    def fmt(a, indent=0):\n",
    "        if a.ndim == 1:\n",
    "            # no 'f' and no casts; compact scientific where needed\n",
    "            return \"{\" + \",\".join(f\"{x:.4f}\" for x in a.tolist()) + \"}\"\n",
    "        body = \",\\n\".join((\" \"*(indent+2)) + fmt(x, indent+2) for x in a)\n",
    "        return \"{\\n\" + body + \"\\n\" + (\" \"*indent) + \"}\"\n",
    "\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"#pragma once\\n\\n\")\n",
    "        f.write(f'{HEADER_INCLUDE}\\n\\n')\n",
    "        f.write(\"// Auto-generated from encoder[0] with BN folded\\n\\n\")\n",
    "        # no 'const' here\n",
    "        f.write(f\"{ctype} {var_name}{dims} = \")\n",
    "        f.write(fmt(array))\n",
    "        f.write(\";\\n\")\n",
    "\n",
    "    # print full path (one-liner)\n",
    "    print(os.path.abspath(out_path))\n",
    "\n",
    "# ---- call these with your already-computed arrays ----\n",
    "# W_hls: (K,K,IN_C,OUT_C), b_fold: (OUT_C,)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "write_header_plain_numbers(W_hls, \"conv0_w\", CTYPE, os.path.join(OUT_DIR, \"encoder_conv0_w.h\"))\n",
    "write_header_plain_numbers(b_fold, \"conv0_b\", CTYPE, os.path.join(OUT_DIR, \"encoder_conv0_b.h\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d78c95fe-3ff5-46ec-b0af-bb2142cb11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WEIGHT COMPARISON"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f6d7d7a-82a7-47c6-987c-b42a5d64012c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Output paths\n",
    "OUT_DIR = \"weights\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "W_TXT_PATH = os.path.join(OUT_DIR, \"encoder0_c1_weights.txt\")\n",
    "B_TXT_PATH = os.path.join(OUT_DIR, \"encoder0_c1_biases.txt\")\n",
    "\n",
    "# Assume these come from your folded BN layer\n",
    "# W_hls.shape = (K, K, IN_C, OUT_C)\n",
    "# b_fold.shape = (OUT_C,)\n",
    "assert W_hls.shape == (3, 3, 3, 32)\n",
    "assert b_fold.shape == (32,)\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Save weights: Flattened as [ky][kx][ic][oc]\n",
    "with open(W_TXT_PATH, \"w\") as f:\n",
    "    for ky in range(3):\n",
    "        for kx in range(3):\n",
    "            for ic in range(3):\n",
    "                for oc in range(32):\n",
    "                    val = W_hls[ky, kx, ic, oc]\n",
    "                    f.write(f\"{val:.4f}\\n\")\n",
    "\n",
    "print(f\"[✓] Wrote flattened weights to: {W_TXT_PATH}\")\n",
    "\n",
    "# ────────────────────────────────────────────────\n",
    "# Save biases\n",
    "with open(B_TXT_PATH, \"w\") as f:\n",
    "    for oc in range(32):\n",
    "        f.write(f\"{b_fold[oc]:.4f}\\n\")\n",
    "\n",
    "print(f\"[✓] Wrote biases to: {B_TXT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd460e-7832-446a-a90f-f133eca8a8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980ad22-4171-4cb2-b9a6-4a2fcf99aad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373e118-9e52-49c8-bb42-982768685671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07c619-61c8-4f24-86ce-cdf52f7df2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d50c8-6dcf-40a0-a0a6-f8e9a0b57a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7caff2d-a0d1-4cd6-9702-566c65ade710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22419345-126c-446b-8df5-f2f90105cd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68498b8-e4d5-49ca-b980-916f7f0575e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee8292cf-76d1-4776-92e2-a9e3b13b949f",
   "metadata": {},
   "source": [
    "### SW output of Conv Stage 0 Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca5295c0-e3a7-472a-ba27-087d5b361272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baron\\AppData\\Local\\Temp\\ipykernel_47472\\2322985082.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved software output for encoder0 layer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# --- Load model ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LaneSegOnlyMobileNetV2(pretrained=False)\n",
    "model.load_state_dict(torch.load(\"lane_seg_weights.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Load input image from .txt file ---\n",
    "def load_txt_image(filepath):\n",
    "    arr = np.loadtxt(filepath, dtype=np.float32)\n",
    "\n",
    "    # Try reshape as (3, 224, 224) first — common channel-first format\n",
    "    if arr.size == 3 * 224 * 224:\n",
    "        arr = arr.reshape(3, 224, 224)\n",
    "        arr = np.transpose(arr, (1, 2, 0))  # (H,W,C) for PIL\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected image shape: flat size {arr.size}\")\n",
    "\n",
    "    return Image.fromarray((arr * 255).astype(np.uint8))\n",
    "\n",
    "# Replace with your path\n",
    "img_path = r\"C:/Users/Baron/Desktop/EE_297_Repo/EE_297/hardware_imp/vitis_hls/lane_seg_hls/test_imgs/img_rgb_224.txt\"\n",
    "img = load_txt_image(img_path)\n",
    "\n",
    "# --- Preprocess image ---\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),  # [0,1], shape: (3,H,W)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "x = preprocess(img).unsqueeze(0).to(device)  # shape: (1, 3, 224, 224)\n",
    "\n",
    "# --- Extract first ConvBNReLU output ---\n",
    "with torch.no_grad():\n",
    "    first_block = model.encoder[0]  # ConvBNReLU: Conv2d → BN → ReLU6\n",
    "    y = first_block(x)              # shape: (1, 32, 112, 112)\n",
    "\n",
    "# --- Save output for comparison ---\n",
    "y_np = y.squeeze(0).cpu().numpy()  # shape: (32, 112, 112)\n",
    "np.save(\"sw_encoder0_output.npy\", y_np)\n",
    "\n",
    "# Optional: save as .txt like hardware\n",
    "with open(\"software_output/sw_encoder0_output.txt\", \"w\") as f:\n",
    "    for c in range(32):\n",
    "        for i in range(112):\n",
    "            for j in range(112):\n",
    "                f.write(f\"{y_np[c][i][j]:.6f} \")\n",
    "            f.write(\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Saved software output for encoder0 layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffefd75-046b-424f-84b2-e0ef71629912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b36e63-b447-4395-a3a9-e2d5bb184322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcdc10-1d1a-4329-a4e3-50b31e824876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
