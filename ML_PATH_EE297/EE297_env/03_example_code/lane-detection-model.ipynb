{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2840219,"sourceType":"datasetVersion","datasetId":1724942}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.loss import _Loss\nfrom torch.autograd import Variable\nimport tqdm\n\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:23:46.203622Z","iopub.execute_input":"2025-04-28T20:23:46.20418Z","iopub.status.idle":"2025-04-28T20:23:46.208237Z","shell.execute_reply.started":"2025-04-28T20:23:46.20416Z","shell.execute_reply":"2025-04-28T20:23:46.207545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print( \"Torch version : {}\".format( str( torch.__version__ )))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:23:48.188996Z","iopub.execute_input":"2025-04-28T20:23:48.189895Z","iopub.status.idle":"2025-04-28T20:23:48.194565Z","shell.execute_reply.started":"2025-04-28T20:23:48.189862Z","shell.execute_reply":"2025-04-28T20:23:48.193779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LaneDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset_path=\"/kaggle/input/tusimple/TUSimple/train_set\", train=True, size=(512, 256)):\n        self._dataset_path = dataset_path\n        self._mode = \"train\" if train else \"eval\"\n        self._image_size = size # w, h\n\n\n        if self._mode == \"train\":\n            label_files = [\n                os.path.join(self._dataset_path, f\"label_data_{suffix}.json\")\n                for suffix in (\"0313\", \"0531\")\n            ]\n        elif self._mode == \"eval\":\n            label_files = [\n                os.path.join(self._dataset_path, f\"label_data_{suffix}.json\")\n                for suffix in (\"0601\",)\n            ]\n\n        self._data = []\n\n        for label_file in label_files:\n            self._process_label_file(label_file)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self._dataset_path, self._data[idx][0])\n        image = cv2.imread(image_path)\n        h, w, c = image.shape\n        #print( \"Image shape : \" + str( image.shape ))\n        raw_image = image\n        image = cv2.resize(image, self._image_size, interpolation=cv2.INTER_LINEAR)\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = image[..., None]\n        lanes = self._data[idx][1]\n\n        segmentation_image = self._draw(h, w, lanes, \"segmentation\")\n        instance_image = self._draw(h, w, lanes, \"instance\")\n\n        instance_image = instance_image[..., None]\n\n        image = torch.from_numpy(image).float().permute((2, 0, 1))\n        segmentation_image = torch.from_numpy(segmentation_image.copy())\n        instance_image =  torch.from_numpy(instance_image.copy()).permute((2, 0, 1))\n        segmentation_image = segmentation_image.to(torch.int64)\n\n        return image, segmentation_image, instance_image, raw_image # 1 x H x W [[0, 1], [2, 0]]\n    \n    def __len__(self):\n        return len(self._data)\n\n    def _draw(self, h, w, lanes, image_type):\n        image = np.zeros((h, w), dtype=np.uint8)\n        for i, lane in enumerate(lanes):\n            color = 1 if image_type == \"segmentation\" else i + 1\n            cv2.polylines(image, [lane], False, color, 10)\n\n        image = cv2.resize(image, self._image_size, interpolation=cv2.INTER_NEAREST)\n\n        return image\n\n    def _process_label_file(self, file_path):\n        with open(file_path) as f:\n            for line in f:\n                info = json.loads(line)\n                image = info[\"raw_file\"]\n                lanes = info[\"lanes\"]\n                h_samples = info[\"h_samples\"]\n                lanes_coords = []\n                for lane in lanes:\n                    x = np.array([lane]).T\n                    y = np.array([h_samples]).T\n                    xy = np.hstack((x, y))\n                    idx = np.where(xy[:, 0] > 0)\n                    lane_coords = xy[idx]\n                    lanes_coords.append(lane_coords)\n                self._data.append((image, lanes_coords))\n                \n    def _show_images_examples( self , number_sample = 10 ):\n        \n        # Visualizing some Lane Detection dataset\n    \n        sns.set_theme()\n\n        f, axarr = plt.subplots( number_sample   ,2 , figsize = ( 20 , 30 ))\n        \n        plt.axis('off')\n\n        for i in range( number_sample ):\n\n            axarr[ i , 0].imshow(  self.__getitem__( idx = i )[ 3 ].reshape( 720 , 1280 , 3)  )\n            axarr[ i , 0 ].set_title( \"Lane Image Data No \" + str( i + 1) )\n            axarr[ i , 0 ].set_axis_off()\n            \n            axarr[ i , 1 ].imshow(  self.__getitem__( idx = i )[ 2 ].reshape( self._image_size ) )\n            axarr[ i , 1 ].set_title( \"Lane Image Segmentation Data No \" + str( i + 1) )\n            axarr[ i , 1 ].set_axis_off()\n\n        f.tight_layout()\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:23:49.974118Z","iopub.execute_input":"2025-04-28T20:23:49.974383Z","iopub.status.idle":"2025-04-28T20:23:49.987364Z","shell.execute_reply.started":"2025-04-28T20:23:49.974364Z","shell.execute_reply":"2025-04-28T20:23:49.986769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class InitialBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - As stated above the number of output channels for this\n        # branch is the total minus 3, since the remaining channels come from\n        # the extension branch\n        self.main_branch = nn.Conv2d(\n            in_channels,\n            out_channels - 1,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=bias)\n\n        # Extension branch\n        self.ext_branch = nn.MaxPool2d(3, stride=2, padding=1)\n\n        # Initialize batch normalization to be used after concatenation\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_activation = activation()\n\n    def forward(self, x):\n        main = self.main_branch(x)\n        ext = self.ext_branch(x)\n\n        # Concatenate branches\n        out = torch.cat((main, ext), 1)\n\n        # Apply batch normalization\n        out = self.batch_norm(out)\n\n        return self.out_activation(out)\n\n\nclass RegularBottleneck(nn.Module):\n    def __init__(self,\n                 channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dilation=1,\n                 asymmetric=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > channels:\n            raise RuntimeError(\"Value out of range. Expected value in the \"\n                               \"interval [1, {0}], got internal_scale={1}.\"\n                               .format(channels, internal_ratio))\n\n        internal_channels = channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - shortcut connection\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution, and,\n        # finally, a regularizer (spatial dropout). Number of channels is constant.\n\n        # 1x1 projection convolution\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                channels,\n                internal_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # If the convolution is asymmetric we split the main convolution in\n        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n        # the first is 5x1 and the second is 1x5.\n        if asymmetric:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(kernel_size, 1),\n                    stride=1,\n                    padding=(padding, 0),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation(),\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(1, kernel_size),\n                    stride=1,\n                    padding=(0, padding),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n        else:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=kernel_size,\n                    stride=1,\n                    padding=padding,\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(channels), activation())\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after adding the branches\n        self.out_activation = activation()\n\n    def forward(self, x):\n        # Main branch shortcut\n        main = x\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_activation(out)\n\n\nclass DownsamplingBottleneck(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 return_indices=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Store parameters that are needed later\n        self.return_indices = return_indices\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(\"Value out of range. Expected value in the \"\n                               \"interval [1, {0}], got internal_scale={1}. \"\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_max1 = nn.MaxPool2d(\n            2,\n            stride=2,\n            return_indices=return_indices)\n\n        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 2x2 projection convolution with stride 2\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                internal_channels,\n                kernel_size=2,\n                stride=2,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # Convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                out_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(out_channels), activation())\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_activation = activation()\n\n    def forward(self, x):\n        # Main branch shortcut\n        if self.return_indices:\n            main, max_indices = self.main_max1(x)\n        else:\n            main = self.main_max1(x)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Main branch channel padding\n        n, ch_ext, h, w = ext.size()\n        ch_main = main.size()[1]\n        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n\n        # Before concatenating, check if main is on the CPU or GPU and\n        # convert padding accordingly\n        if main.is_cuda:\n            padding = padding.cuda()\n        \n        #padding = padding.cpu()\n\n        # Concatenate\n        main = torch.cat((main, padding), 1)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_activation(out), max_indices\n\n\nclass UpsamplingBottleneck(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(\"Value out of range. Expected value in the \"\n                               \"interval [1, {0}], got internal_scale={1}. \"\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels))\n\n        # Remember that the stride is the same as the kernel_size, just like\n        # the max pooling layers\n        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 1x1 projection convolution with stride 1\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels, internal_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(internal_channels), activation())\n\n        # Transposed convolution\n        self.ext_tconv1 = nn.ConvTranspose2d(\n            internal_channels,\n            internal_channels,\n            kernel_size=2,\n            stride=2,\n            bias=bias)\n        self.ext_tconv1_bnorm = nn.BatchNorm2d(internal_channels)\n        self.ext_tconv1_activation = activation()\n\n        # 1x1 expansion convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels), activation())\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_activation = activation()\n\n    def forward(self, x, max_indices, output_size):\n        # Main branch shortcut\n        main = self.main_conv1(x)\n        main = self.main_unpool1(\n            main, max_indices, output_size=output_size)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_tconv1(ext, output_size=output_size)\n        ext = self.ext_tconv1_bnorm(ext)\n        ext = self.ext_tconv1_activation(ext)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_activation(out)\n\n\nclass ENet(nn.Module):\n    def __init__(self, binary_seg, embedding_dim, encoder_relu=False, decoder_relu=True):\n        super(ENet, self).__init__()\n\n        self.initial_block = InitialBlock(1, 16, relu=encoder_relu)\n\n        # Stage 1 share\n        self.downsample1_0 = DownsamplingBottleneck(16, 64, return_indices=True, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_1 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_2 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_3 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_4 = RegularBottleneck(64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n\n        # Stage 2 share\n        self.downsample2_0 = DownsamplingBottleneck(64, 128, return_indices=True, dropout_prob=0.1, relu=encoder_relu)\n        self.regular2_1 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_2 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_3 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_4 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular2_5 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_6 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_7 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_8 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # stage 3 binary\n        self.regular_binary_3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_binary_3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric_binary_3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_binary_3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular_binary_3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_binary_3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric_binary_3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_binary_3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # stage 3 embedding\n        self.regular_embedding_3_0 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_embedding_3_1 = RegularBottleneck(128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric_embedding_3_2 = RegularBottleneck(128, kernel_size=5, padding=2, asymmetric=True, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_embedding_3_3 = RegularBottleneck(128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular_embedding_3_4 = RegularBottleneck(128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_embedding_3_5 = RegularBottleneck(128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric_bembedding_3_6 = RegularBottleneck(128, kernel_size=5, asymmetric=True, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated_embedding_3_7 = RegularBottleneck(128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # binary branch\n        self.upsample_binary_4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.1, relu=decoder_relu)\n        self.regular_binary_4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular_binary_4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.upsample_binary_5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.1, relu=decoder_relu)\n        self.regular_binary_5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.binary_transposed_conv = nn.ConvTranspose2d(16, binary_seg, kernel_size=3, stride=2, padding=1, bias=False)\n\n        # embedding branch\n        self.upsample_embedding_4_0 = UpsamplingBottleneck(128, 64, dropout_prob=0.1, relu=decoder_relu)\n        self.regular_embedding_4_1 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular_embedding_4_2 = RegularBottleneck(64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.upsample_embedding_5_0 = UpsamplingBottleneck(64, 16, dropout_prob=0.1, relu=decoder_relu)\n        self.regular_embedding_5_1 = RegularBottleneck(16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.embedding_transposed_conv = nn.ConvTranspose2d(16, embedding_dim, kernel_size=3, stride=2, padding=1, bias=False)\n\n    def forward(self, x):\n        # Initial block\n        input_size = x.size()\n        x = self.initial_block(x)\n\n        # Stage 1 share\n        stage1_input_size = x.size()\n        x, max_indices1_0 = self.downsample1_0(x)\n        x = self.regular1_1(x)\n        x = self.regular1_2(x)\n        x = self.regular1_3(x)\n        x = self.regular1_4(x)\n\n        # Stage 2 share\n        stage2_input_size = x.size()\n        x, max_indices2_0 = self.downsample2_0(x)\n        x = self.regular2_1(x)\n        x = self.dilated2_2(x)\n        x = self.asymmetric2_3(x)\n        x = self.dilated2_4(x)\n        x = self.regular2_5(x)\n        x = self.dilated2_6(x)\n        x = self.asymmetric2_7(x)\n        x = self.dilated2_8(x)\n\n        # stage 3 binary\n        x_binary = self.regular_binary_3_0(x)\n        x_binary = self.dilated_binary_3_1(x_binary)\n        x_binary = self.asymmetric_binary_3_2(x_binary)\n        x_binary = self.dilated_binary_3_3(x_binary)\n        x_binary = self.regular_binary_3_4(x_binary)\n        x_binary = self.dilated_binary_3_5(x_binary)\n        x_binary = self.asymmetric_binary_3_6(x_binary)\n        x_binary = self.dilated_binary_3_7(x_binary)\n\n        # stage 3 embedding\n        x_embedding = self.regular_embedding_3_0(x)\n        x_embedding = self.dilated_embedding_3_1(x_embedding)\n        x_embedding = self.asymmetric_embedding_3_2(x_embedding)\n        x_embedding = self.dilated_embedding_3_3(x_embedding)\n        x_embedding = self.regular_embedding_3_4(x_embedding)\n        x_embedding = self.dilated_embedding_3_5(x_embedding)\n        x_embedding = self.asymmetric_bembedding_3_6(x_embedding)\n        x_embedding = self.dilated_embedding_3_7(x_embedding)\n\n        # binary branch\n        x_binary = self.upsample_binary_4_0(x_binary, max_indices2_0, output_size=stage2_input_size)\n        x_binary = self.regular_binary_4_1(x_binary)\n        x_binary = self.regular_binary_4_2(x_binary)\n        x_binary = self.upsample_binary_5_0(x_binary, max_indices1_0, output_size=stage1_input_size)\n        x_binary = self.regular_binary_5_1(x_binary)\n        binary_final_logits = self.binary_transposed_conv(x_binary, output_size=input_size)\n\n        # embedding branch\n        x_embedding = self.upsample_embedding_4_0(x_embedding, max_indices2_0, output_size=stage2_input_size)\n        x_embedding = self.regular_embedding_4_1(x_embedding)\n        x_embedding = self.regular_embedding_4_2(x_embedding)\n        x_embedding = self.upsample_embedding_5_0(x_embedding, max_indices1_0, output_size=stage1_input_size)\n        x_embedding = self.regular_embedding_5_1(x_embedding)\n        instance_final_logits = self.embedding_transposed_conv(x_embedding, output_size=input_size)\n\n        return binary_final_logits, instance_final_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:23:53.529872Z","iopub.execute_input":"2025-04-28T20:23:53.530422Z","iopub.status.idle":"2025-04-28T20:23:53.567461Z","shell.execute_reply.started":"2025-04-28T20:23:53.530402Z","shell.execute_reply":"2025-04-28T20:23:53.566785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DiscriminativeLoss(_Loss):\n    def __init__(self, delta_var=0.5, delta_dist=3,\n                 norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n                 device=\"cpu\", reduction=\"mean\", n_clusters=4):\n        super(DiscriminativeLoss, self).__init__(reduction=reduction)\n        self.delta_var = delta_var\n        self.delta_dist = delta_dist\n        self.norm = norm\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.device = torch.device(device)\n        self.n_clusters = n_clusters\n        assert self.norm in [1, 2]\n\n    def forward(self, input, target):\n        assert not target.requires_grad\n\n        return self._discriminative_loss(input, target)\n\n    def _discriminative_loss(self, input, target):\n        num_samples=target.size(0)\n\n        dis_loss=torch.tensor(0.).to(self.device)\n        var_loss=torch.tensor(0.).to(self.device)\n        reg_loss=torch.tensor(0.).to(self.device)\n        for i in range(num_samples):\n            clusters=[]\n            sample_embedding=input[i,:,:,:]\n            sample_label=target[i,:,:].squeeze()\n            num_clusters=len(sample_label.unique())-1\n            vals=sample_label.unique()[1:]\n            sample_label=sample_label.view(sample_label.size(0)*sample_label.size(1))\n            sample_embedding=sample_embedding.view(-1,sample_embedding.size(1)*sample_embedding.size(2))\n            v_loss=torch.tensor(0.).to(self.device)\n            d_loss=torch.tensor(0.).to(self.device)\n            r_loss=torch.tensor(0.).to(self.device)\n            for j in range(num_clusters):\n                indices=(sample_label==vals[j]).nonzero()\n                indices=indices.squeeze()\n                cluster_elements=torch.index_select(sample_embedding,1,indices)\n                Nc=cluster_elements.size(1)\n                mean_cluster=cluster_elements.mean(dim=1,keepdim=True)\n                clusters.append(mean_cluster)\n                v_loss+=torch.pow((torch.clamp(torch.norm(cluster_elements-mean_cluster)-self.delta_var,min=0.)),2).sum()/Nc\n                r_loss+=torch.sum(torch.abs(mean_cluster))\n            for index in range(num_clusters):\n                for idx,cluster in enumerate(clusters):\n                    if index==idx:\n                        continue \n                    else:\n                        distance=torch.norm(clusters[index]-cluster)#torch.sqrt(torch.sum(torch.pow(clusters[index]-cluster,2)))\n                        d_loss+=torch.pow(torch.clamp(self.delta_dist-distance,min=0.),2)\n            var_loss+=v_loss/num_clusters\n            dis_loss+=d_loss/(num_clusters*(num_clusters-1))\n            reg_loss+=r_loss/num_clusters\n        return self.alpha*(var_loss/num_samples)+self.beta*(dis_loss/num_samples)+self.gamma*(reg_loss/num_samples)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:24:01.190625Z","iopub.execute_input":"2025-04-28T20:24:01.190904Z","iopub.status.idle":"2025-04-28T20:24:01.202635Z","shell.execute_reply.started":"2025-04-28T20:24:01.190884Z","shell.execute_reply":"2025-04-28T20:24:01.201794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DiscriminativeLoss(_Loss):\n    def __init__(self, delta_var=0.5, delta_dist=3,\n                 norm=2, alpha=1.0, beta=1.0, gamma=0.001,\n                 device=\"cpu\", reduction=\"mean\", n_clusters=4):\n        super(DiscriminativeLoss, self).__init__(reduction=reduction)\n        self.delta_var = delta_var\n        self.delta_dist = delta_dist\n        self.norm = norm\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.device = torch.device(device)\n        self.n_clusters = n_clusters\n        assert self.norm in [1, 2]\n\n    def forward(self, input, target):\n        assert not target.requires_grad\n\n        return self._discriminative_loss(input, target)\n\n    def _discriminative_loss(self, input, target):\n        num_samples=target.size(0)\n\n        dis_loss=torch.tensor(0.).to(self.device)\n        var_loss=torch.tensor(0.).to(self.device)\n        reg_loss=torch.tensor(0.).to(self.device)\n        for i in range(num_samples):\n            clusters=[]\n            sample_embedding=input[i,:,:,:]\n            sample_label=target[i,:,:].squeeze()\n            num_clusters=len(sample_label.unique())-1\n            vals=sample_label.unique()[1:]\n            sample_label=sample_label.view(sample_label.size(0)*sample_label.size(1))\n            sample_embedding=sample_embedding.view(-1,sample_embedding.size(1)*sample_embedding.size(2))\n            v_loss=torch.tensor(0.).to(self.device)\n            d_loss=torch.tensor(0.).to(self.device)\n            r_loss=torch.tensor(0.).to(self.device)\n            for j in range(num_clusters):\n                indices=(sample_label==vals[j]).nonzero()\n                indices=indices.squeeze()\n                cluster_elements=torch.index_select(sample_embedding,1,indices)\n                Nc=cluster_elements.size(1)\n                mean_cluster=cluster_elements.mean(dim=1,keepdim=True)\n                clusters.append(mean_cluster)\n                v_loss+=torch.pow((torch.clamp(torch.norm(cluster_elements-mean_cluster)-self.delta_var,min=0.)),2).sum()/Nc\n                r_loss+=torch.sum(torch.abs(mean_cluster))\n            for index in range(num_clusters):\n                for idx,cluster in enumerate(clusters):\n                    if index==idx:\n                        continue \n                    else:\n                        distance=torch.norm(clusters[index]-cluster)#torch.sqrt(torch.sum(torch.pow(clusters[index]-cluster,2)))\n                        d_loss+=torch.pow(torch.clamp(self.delta_dist-distance,min=0.),2)\n            var_loss+=v_loss/num_clusters\n            dis_loss+=d_loss/(num_clusters*(num_clusters-1))\n            reg_loss+=r_loss/num_clusters\n        return self.alpha*(var_loss/num_samples)+self.beta*(dis_loss/num_samples)+self.gamma*(reg_loss/num_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:24:04.917208Z","iopub.execute_input":"2025-04-28T20:24:04.917471Z","iopub.status.idle":"2025-04-28T20:24:04.92804Z","shell.execute_reply.started":"2025-04-28T20:24:04.917451Z","shell.execute_reply":"2025-04-28T20:24:04.927198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_loss(binary_output, instance_output, binary_label, instance_label):\n    ce_loss = nn.CrossEntropyLoss()\n    binary_loss = ce_loss(binary_output, binary_label)\n\n    ds_loss = DiscriminativeLoss(delta_var=0.5, delta_dist=3, alpha=1.0, beta=1.0, gamma=0.001, \n                                 device= \"cuda\" )#device=\"cpu\")\n    instance_loss = ds_loss(instance_output, instance_label)\n    \n    return binary_loss, instance_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:24:07.28721Z","iopub.execute_input":"2025-04-28T20:24:07.287707Z","iopub.status.idle":"2025-04-28T20:24:07.291947Z","shell.execute_reply.started":"2025-04-28T20:24:07.287681Z","shell.execute_reply":"2025-04-28T20:24:07.291236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show tensorflow\n!nvcc --version\n!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:24:11.522107Z","iopub.execute_input":"2025-04-28T20:24:11.522667Z","iopub.status.idle":"2025-04-28T20:24:13.867855Z","shell.execute_reply.started":"2025-04-28T20:24:11.522646Z","shell.execute_reply":"2025-04-28T20:24:13.867076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tqdm\nimport os\n\n# Define your LaneDataset class, ENet model, DiscriminativeLoss, and compute_loss as provided earlier\n\n# Constants for training\nBATCH_SIZE = 16\nLR = 5e-4\nNUM_EPOCHS = 128 #32\n\n# Create the training dataset and dataloader\ntrain_dataset = LaneDataset()\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint( \"Is Torch package is CUDA enabled : \" + str( torch.cuda.is_available() ))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#Using CPU for easy usage\n#device = \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:25:41.036965Z","iopub.execute_input":"2025-04-28T20:25:41.03774Z","iopub.status.idle":"2025-04-28T20:25:41.321639Z","shell.execute_reply.started":"2025-04-28T20:25:41.037712Z","shell.execute_reply":"2025-04-28T20:25:41.320878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset._show_images_examples( number_sample= 15 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:25:48.094362Z","iopub.execute_input":"2025-04-28T20:25:48.094704Z","iopub.status.idle":"2025-04-28T20:25:53.443333Z","shell.execute_reply.started":"2025-04-28T20:25:48.094684Z","shell.execute_reply":"2025-04-28T20:25:53.442472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the ENet model and move it to the GPU device\nenet_model = ENet(2, 8)\nenet_model.to(device)\n\n# Define the optimizer\nparams = [p for p in enet_model.parameters() if p.requires_grad]\noptimizer = torch.optim.Adam(params, lr=LR, weight_decay=0.0002)\n\n# Create a directory for logs\nlog_dir = \"logs\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Set up TensorBoard writer\nwriter = SummaryWriter(log_dir=log_dir)\n\n# Lists to store losses and accuracies\nbinary_losses_epoch = []\ninstance_losses_epoch = []\ntrain_accuracies = []\ntrain_f1_score = []\n\n# Training loop\nfor epoch in range(NUM_EPOCHS):\n    enet_model.train()\n    losses = []\n    correct_binary = 0\n    total_pixels = 0\n    false_positive_result= 0\n    true_positive_result = 0\n    \n    for batch in tqdm.tqdm(train_dataloader):\n        img, binary_target, instance_target, raw_image = batch\n        img = img.to(device)\n        binary_target = binary_target.to(device)\n        instance_target = instance_target.to(device)\n\n        optimizer.zero_grad()\n\n        binary_logits, instance_emb = enet_model(img)\n\n        binary_loss, instance_loss = compute_loss(binary_logits, instance_emb, binary_target, instance_target)\n        loss = binary_loss + instance_loss\n        loss.backward()\n\n        optimizer.step()\n\n        losses.append((binary_loss.detach().cpu(), instance_loss.detach().cpu()))\n\n        binary_preds = torch.argmax(binary_logits, dim=1)\n        correct_binary += torch.sum(binary_preds == binary_target).item()\n        \n        false_positive_result += torch.sum( ( binary_preds == 1 ) & ( binary_target == 0 ) ).item()\n        true_positive_result += torch.sum( (binary_preds == 1 ) & ( binary_target == 1 ) ).item()\n        total_pixels += binary_target.numel()\n\n    binary_accuracy = correct_binary / total_pixels\n    binary_total_false = total_pixels - correct_binary\n    binary_precision = ( true_positive_result )/ ( true_positive_result + false_positive_result )\n    binary_recall = ( true_positive_result )/ ( true_positive_result + binary_total_false - false_positive_result )\n    binary_f1_score = 0 if ( binary_recall == 0 ) or ( binary_precision == 0 ) else 2/ ( 1/binary_precision + 1/binary_recall )\n    train_accuracies.append(binary_accuracy)\n    train_f1_score.append( binary_f1_score )\n\n    mean_losses = np.array(losses).mean(axis=0)\n    binary_losses_epoch.append(mean_losses[0])\n    instance_losses_epoch.append(mean_losses[1])\n\n    # Log metrics to TensorBoard\n    writer.add_scalar(\"Binary Loss\", mean_losses[0], epoch)\n    writer.add_scalar(\"Instance Loss\", mean_losses[1], epoch)\n    writer.add_scalar(\"Binary Accuracy\", binary_accuracy, epoch)\n    writer.add_scalar( \"Binary F1 Score\", binary_f1_score , epoch )\n    \n\n    # Log details of all layers in histogram format\n    for name, param in enet_model.named_parameters():\n        writer.add_histogram(name, param.clone().cpu().data.numpy(), global_step=epoch)\n\n    # Print and save results for this epoch\n    msg = (f\"Epoch {epoch}:\"\n          f\" Binary Loss = {mean_losses[0]:.4f}, Instance Loss = {mean_losses[1]:.4f}, Binary Accuracy = {binary_accuracy:.4f} , Binary F1- Score = {binary_f1_score:.4f}\" )\n    print(msg)\n\n# Close TensorBoard writer\nwriter.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T20:26:06.966263Z","iopub.execute_input":"2025-04-28T20:26:06.967007Z","iopub.status.idle":"2025-04-28T23:40:52.64072Z","shell.execute_reply.started":"2025-04-28T20:26:06.96698Z","shell.execute_reply":"2025-04-28T23:40:52.639554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(enet_model, \"lane_detection_model_full.pth\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}